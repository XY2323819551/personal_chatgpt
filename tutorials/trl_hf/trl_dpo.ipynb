{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa570992-7300-4e41-a673-f54a828b562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a575267-4a0c-47ee-8fdc-a53533f4a0e4",
   "metadata": {},
   "source": [
    "- misc\n",
    "    - https://github.com/huggingface/alignment-handbook/tree/main\n",
    "- 3 steps\n",
    "    - pre-training a large language model (LLM) to predict the next token on internet-scale data, on clusters of thousands of GPUs. One calls the result a **\"base model\"**\n",
    "    - supervised fine-tuning (SFT) to turn the base model into a useful assistant (ChatBot)\n",
    "        - we turned a \"base model\" into a useful assistant, by training it to **generate useful completions given human instructions.**\n",
    "    - human preference fine-tuning which increases the assistant's friendliness, helpfulness and safety.\n",
    "        - \"safe\", \"friendly\", \"harmless\", \"inclusive\",\n",
    "        - human preference fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74ea9ae-cabd-46d4-8041-10d0d5c7f768",
   "metadata": {},
   "source": [
    "## align & why align"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8139c761-3081-4fa0-8c22-06d7f4867863",
   "metadata": {},
   "source": [
    "- collect human/ai feedback to learn $p(y_w\\gt y_l)$\n",
    "- RLHF - the OG（Original Gangster，始祖） of LLM alignment\n",
    "\n",
    "    $$\n",
    "    \\max_{\\pi_\\theta} \\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim \\pi_\\theta(y \\mid x)} \\underbrace{\\left[ r_\\phi(x, y) \\right]}_{\\text{maximise rewards}} - \\underbrace{\\beta \\mathbb{D}_{\\text{KL}} \\left[ \\pi_\\theta(y \\mid x) \\parallel \\pi_{\\text{ref}}(y \\mid x) \\right]}_{\\text{use KL penalty to prevent\n",
    "    reward hacking (controlled by β)\n",
    "    }}\n",
    "    $$\n",
    "    - RL（PPO）很多超参，且训练不稳定；\n",
    "    - 还需要一个RM（Reward model），这样的话一共三个model要操作，actor model，ref model，Reward model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a086b96-47b3-4f68-b6b0-daae6f0f4665",
   "metadata": {},
   "source": [
    "## DPO（Direct Preference Optimization）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff27b752-39a3-4887-b7eb-6616e1257fbc",
   "metadata": {},
   "source": [
    "$$\n",
    "\\max_{\\pi} \\mathbb{E}_{(x, y_w, y_l) \\sim D} \\log \\sigma \\left( \\beta \\log \\frac{\\pi(y_w | x)}{\\pi_{\\text{ref}}(y_w | x)} - \\beta \\log \\frac{\\pi(y_l | x)}{\\pi_{\\text{ref}}(y_l | x)} \\right)\n",
    "$$\n",
    "- only two models (actor/active model, reference model (sft))\n",
    "- 求导练习\n",
    "\n",
    "    $$\n",
    "    \\left(\\log\\sigma(z)\\right))'=\\frac{1}{\\sigma(z)}\\cdot \\sigma(z)(1-\\sigma(z))=1-\\sigma(z)=\\sigma(-z)\n",
    "    $$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta} \\mathcal{L}_{\\text{DPO}} (\\pi_{\\theta}; \\pi_{\\text{ref}}) = & -\\beta \\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\underbrace{\\sigma \\left( \\hat{r}_{\\theta}(x, y_l) - \\hat{r}_{\\theta}(x, y_w) \\right)}_{\\text{higher weight when reward estimate is wrong} } \\left[ \\underbrace{\\nabla_{\\theta} \\log \\pi(y_w | x)}_{\\text{increase likelihood of } y_w} - \\underbrace{\\nabla_{\\theta} \\log \\pi(y_l | x)}_{\\text{decrease likelihood of } y_l} \\right] \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- $\\hat r_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}$（implicit reward from LM）\n",
    "    - 它表示的是模型 $\\pi_\\theta$ 相对于参考模型 $\\pi_{\\text{ref}}$ 对生成结果 $y$ 的偏好程度。\n",
    "    - 与显式奖励（例如通过人工评分或者明确的奖励函数给出的奖励）不同，隐式奖励是通过模型内部的概率分布计算得到的。在DPO中，这种隐式奖励直接来源于模型本身的输出概率分布，因此称为“隐式奖励”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf81006e-6215-480f-9a76-235ed38ae224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F487af2f0-e51d-4140-92a7-23476c5ea016_1600x1015.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F487af2f0-e51d-4140-92a7-23476c5ea016_1600x1015.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1869cb8-02bf-4671-8f7e-53dcd31af57b",
   "metadata": {},
   "source": [
    "## practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2cbb16-2856-47d0-a274-705588b09825",
   "metadata": {},
   "source": [
    "- https://colab.research.google.com/drive/1mWiOFBy3zY6OdINEvHN9EPoQ_VIvfFKw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1f5bcd7-90a5-488c-89f9-3f2d7ddcf8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flash-attn --no-build-isolation\n",
    "import os\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1996466-c725-46bf-b835-72fe1d2990b4",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f962b140-8ed6-4fd5-aa9e-b7b4abb7bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d8f1089-9aa0-47e1-aa08-a27f003bb2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_prefs: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
       "        num_rows: 61135\n",
       "    })\n",
       "    train_sft: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
       "        num_rows: 61135\n",
       "    })\n",
       "    test_prefs: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test_sft: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_gen: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
       "        num_rows: 61135\n",
       "    })\n",
       "    test_gen: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gen, sft, prefs\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf97a3ec-b817-401d-8b13-615b96559fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "indices = range(0,100)\n",
    "\n",
    "dataset_dict = {\"train\": raw_datasets[\"train_prefs\"].select(indices),\n",
    "                \"test\": raw_datasets[\"test_prefs\"].select(indices)}\n",
    "\n",
    "raw_datasets = DatasetDict(dataset_dict)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78d8e04e-e5d2-420d-b610-32c1bd663a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how can i develop a habit of drawing daily\n",
      "=================\n",
      "Developing a daily habit of drawing can be challenging but with consistent practice and a few tips, it can become an enjoyable and rewarding part of your daily routine. Here are some strategies to help you develop the habit of drawing daily:\n",
      "\n",
      "1. Set a specific time: Allocate a specific time of the day to draw. It could be in the morning, afternoon, or evening. Make drawing a part of your daily routine.\n",
      "2. Set a specific duration: Determine the amount of time you want to spend on drawing each day. It can be as little as 10 minutes or as long as an hour. Be consistent with the duration to help build the habit.\n",
      "3. Start small and simple: Don't try to create a masterpiece every day, start with simple and easy-to-do sketches. Focus on improving your skills gradually.\n",
      "4. Use a variety of tools and mediums: Experiment with different tools like pencils, pens, markers, and different mediums like paper, canvas, or digital apps to keep your drawing practice interesting and engaging.\n",
      "5. Take breaks and rest: Taking breaks and resting after some time of drawing can help you avoid burnout and stay motivated.\n",
      "6. Challenge yourself: Set challenges like drawing objects from memory or a specific subject to improve your skills and keep your drawing practice interesting.\n",
      "7. Track your progress: Keep a record of your daily drawing practice and track your progress. This can be a source of motivation and help you see how far you've come.\n",
      "\n",
      "Remember, developing a habit takes time and patience. Stay consistent with your drawing practice, be flexible and open to trying new things, and with time, you'll develop a habit of daily drawing that brings you joy and satisfaction.\n",
      "=================\n",
      "As an AI language model, I cannot personally develop habits for you. But, here are some tips for developing a habit of drawing daily:\n",
      "\n",
      "1. Start small: Start with simple drawings or doodles and gradually increase the complexity of your drawings.\n",
      "\n",
      "2. Set a schedule: Set a specific time every day to draw, and try to stick to it as much as possible.\n",
      "\n",
      "3. Make it fun: Don't pressure yourself to create masterpieces every time you draw. Make it a relaxing and enjoyable experience.\n",
      "\n",
      "4. Use resources: There are many drawing tutorials available online. Use resources like YouTube or online drawing courses to help you improve your skills.\n",
      "\n",
      "5. Surround yourself with inspiration: Expose yourself to a variety of art forms, such as paintings, illustrations, and photographs, to inspire and motivate you.\n",
      "\n",
      "Remember, everyone has their own creative style and pace. Just keep practicing and enjoying the process of drawing.\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets['train'][0]['prompt'])\n",
    "print('=================')\n",
    "print(raw_datasets['train'][0]['chosen'][1]['content'])\n",
    "print('=================')\n",
    "print(raw_datasets['train'][0]['rejected'][1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02f626d-4ecf-4cc3-8a9d-eb2adf4c4e3d",
   "metadata": {},
   "source": [
    "### tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1e5fa62-91c2-4dbd-be65-2c022041ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"alignment-handbook/zephyr-7b-sft-lora\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fade102-7421-4bf5-8cbf-e98619f29f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'left'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3319456b-d501-455b-aded-e86b16530d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '</s>'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc88f825-ff0a-4ff0-8d9f-440876fbac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate from left to ensure we don't lose labels in final turn\n",
    "tokenizer.truncation_side = \"left\"\n",
    "\n",
    "# Set reasonable default for models without max length\n",
    "if tokenizer.model_max_length > 100_000:\n",
    "    tokenizer.model_max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fccdf39e-df26-4b62-9bf6-3300ff7e212c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1b932c6-b6dd-4559-a684-6fa662f9602e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "print(DEFAULT_CHAT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3be51ab7-7284-4a21-bd60-297637b6981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77a5763-9d35-4a0e-a27e-4c05cd9d1762",
   "metadata": {},
   "source": [
    "### Apply chat template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11e8504-8095-47ea-ab62-a1dc039202d5",
   "metadata": {},
   "source": [
    "- 用于奖励建模或区分正例和负例\n",
    "- two must keys\n",
    "    - chosen\n",
    "    - rejected\n",
    "- `</s>`: eos_token, pad_token\n",
    "- system, user, assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f7f0d95-5490-4bc1-8973-6551049e9987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def apply_chat_template(example, tokenizer, assistant_prefix=\"<|assistant|>\\n\"):\n",
    "    def _strip_prefix(s, pattern):\n",
    "        # Use re.escape to escape any special characters in the pattern\n",
    "        return re.sub(f\"^{re.escape(pattern)}\", \"\", s)\n",
    "\n",
    "    if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n",
    "            # Compared to reward modeling, we filter out the prompt, so the text is everything after the last assistant token\n",
    "            prompt_messages = [[msg for msg in example[\"chosen\"] if msg[\"role\"] == \"user\"][0]]\n",
    "            # Insert system message\n",
    "            if example[\"chosen\"][0][\"role\"] != \"system\":\n",
    "                prompt_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "            else:\n",
    "                prompt_messages.insert(0, example[\"chosen\"][0])\n",
    "            # TODO: handle case where chosen/rejected also have system messages\n",
    "            chosen_messages = example[\"chosen\"][1:]\n",
    "            rejected_messages = example[\"rejected\"][1:]\n",
    "            example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n",
    "            example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
    "            example[\"text_prompt\"] = tokenizer.apply_chat_template(\n",
    "                prompt_messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            example[\"text_chosen\"] = _strip_prefix(example[\"text_chosen\"], assistant_prefix)\n",
    "            example[\"text_rejected\"] = _strip_prefix(example[\"text_rejected\"], assistant_prefix)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Could not format example as dialogue for `dpo` task! Require `[chosen, rejected]` keys but found {list(example.keys())}\"\n",
    "        )\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d248a2ab-a178-4d38-8902-6148ec184435",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = apply_chat_template(raw_datasets['train'][0], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f8dac1f-4c37-4f77-a10e-ad1d63dad0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "how can i develop a habit of drawing daily</s>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test['text_prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f6cbe79-1dd9-428f-bbed-be261e8a795a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prompt',\n",
       " 'prompt_id',\n",
       " 'chosen',\n",
       " 'rejected',\n",
       " 'messages',\n",
       " 'score_chosen',\n",
       " 'score_rejected']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(raw_datasets['train'].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "430a063e-45d6-42db-9409-fb88ff8eaf98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text_chosen', 'text_rejected', 'text_prompt'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text_chosen', 'text_rejected', 'text_prompt'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "column_names = list(raw_datasets[\"train\"].features)\n",
    "\n",
    "raw_datasets = raw_datasets.map(\n",
    "        apply_chat_template,\n",
    "        fn_kwargs={\"tokenizer\": tokenizer},\n",
    "        num_proc=cpu_count(),\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Formatting comparisons with prompt template\",\n",
    ")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80c7692a-6d7b-42e5-8d63-59f0f625f848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['chosen', 'rejected', 'prompt'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['chosen', 'rejected', 'prompt'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace column names with what TRL needs, text_chosen -> chosen and text_rejected -> rejected\n",
    "for split in [\"train\", \"test\"]:\n",
    "    raw_datasets[split] = raw_datasets[split].rename_columns(\n",
    "        {\"text_prompt\": \"prompt\", \"text_chosen\": \"chosen\", \"text_rejected\": \"rejected\"}\n",
    "    )\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94df4fb6-951f-4e1e-bfca-07e3c89c8ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt sample 98 of the raw training set:\n",
      "\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "In the year 1630 during the month of January Captain Cook began his voyage from the port of Calais in Europe, which is in the Northern Hemisphere. His intention was  to find the mythical land in the Southern Hemisphere. After about eleven months of sailing, in December 1630, he reached Australia, which is indeed in the Southern Hemisphere. Thinking he finally reached the mythical  land, he commenced his return voyage to Europe on June,  1631, He reached Europe in the middle of the following year on July, 1632.  Given the paragraph above, please answer correctly the following question:   Did Australia exprience increased or decreased solar flux when Captain Cook reached Australia in 1630?\n",
      "----\n",
      "Answer: increased\n",
      "\n",
      "\n",
      "David is an environmental scientist. He needed to find causes of wildfires and suggest preventive measures. First, he visited a dense forest. He marked it as location A. Then he visited a grassland, which he marked as location B. After that, he visited a location where he did not find any sign of drought. He marked it as time A. He then visited the same location a year later, and found that the place was facing a severe drought. He marked it as time B.  Given the paragraph above, please answer correctly the following question:   Would location B be less or more susceptible to wildfires than location A?\n",
      "----\n",
      "Answer: more\n",
      "\n",
      "\n",
      "Two neighboring towns used different approaches to agriculture. Big town used pesticides and fertilizers, while Small town applied permaculture principles and organic farming methods.  Given the paragraph above, please answer correctly the following question:   Which town had less growth of water plants because of fertilizer?\n",
      "----\n",
      "Answer:</s>\n",
      "<|assistant|>\n",
      "\n",
      "Chosen sample 98 of the raw training set:\n",
      "\n",
      "Big town</s>\n",
      "\n",
      "Rejected sample 98 of the raw training set:\n",
      "\n",
      "Sure, I can answer those questions:\n",
      "\n",
      "1. According to the given information, it seems like solar flux in Australia was likely higher when Captain Cook reached there in 1630. Increased solar flux can lead to higher temperatures and more intense solar radiation, which can increase the chances of wildfires. However, there is no way to know for sure without further information about the environment in Australia at that time. It's also possible that other environmental factors contributed to the increased risk of wildfires, such as changing weather patterns or the presence of dry lightning strikes.\n",
      "\n",
      "2. Based on the given information, location B would be more susceptible to wildfires than location A. Wildfires tend to spread quickly and easily in grasslands, which is what location B appears to be. Grasslands also have thin vegetation and lower levels of trees and shrubs, which can contribute to the rapid spread of fires. Additionally, location B is more vulnerable to drought conditions, which can further increase the risk of wildfires.\n",
      "\n",
      "3. According to the given information, it's likely that Big Town had higher levels of pollution and pesticides than Small Town, which could have led to health problems and damage to the environment. However, without additional information about the specifics of each town's agriculture practices, it's difficult to say for sure which approach was more environmentally friendly.</s>\n",
      "\n",
      "Prompt sample 74 of the raw training set:\n",
      "\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "Create an e-learning platform in Laravel that offers online courses with multimedia content and interactive quizzes.</s>\n",
      "<|assistant|>\n",
      "\n",
      "Chosen sample 74 of the raw training set:\n",
      "\n",
      "Creating an e-learning platform in Laravel with online courses, multimedia content, and interactive quizzes involves several steps:\n",
      "\n",
      "1. **Setting up the Environment:**\n",
      "   Install the necessary prerequisites including PHP, Laravel, and MySQL. Laravel uses Composer (a PHP dependency manager) and Node.js (for some features likemix).\n",
      "\n",
      "2. **Creating a New Laravel Project:**\n",
      "   Use the command `composer create-project` to create a new Laravel project.\n",
      "\n",
      "3. **Setting up the Database:**\n",
      "   Edit the .env file with your DB credentials and then run `php artisan migrate`. The migration command will create a table in your database based on the schema you defined in your migration file.\n",
      "\n",
      "4. **Creating Models:**\n",
      "   Use the `php artisan make:model` command to create models for your database tables. In your case, you might need models for 'Users', 'Courses', 'Lectures', 'Quizzes', 'Answers', etc.\n",
      "\n",
      "5. **Defining Relationships:**\n",
      "   Laravel uses Eloquent ORM for database querying. Define relationships between models like User and Course with methods like `belongsTo`, `hasMany`, etc.\n",
      "\n",
      "6. **Creating Controllers:**\n",
      "   Use the `php artisan make:controller` command to create controllers for your routes. You'll need controllers for users to create, update, and delete courses, lectures, and quizzes.\n",
      "\n",
      "7. **Setting up Routes:**\n",
      "   Define the routes to the controllers in routes/web.php. In Laravel, you can use URI, middleware, and HTTP methods to define routes.\n",
      "\n",
      "8. **Creating Views:**\n",
      "   Create the view files in the 'resources/views' directory. You'll need views for displaying courses, creating/updating courses, taking quizzes, and displaying quiz results.\n",
      "\n",
      "9. **Uploading Multimedia Content:**\n",
      "   Laravel provides a Stream link, which can be passed to the browser to display files as if they were a website. You can use this to display images, videos, and audio.\n",
      "\n",
      "10. **Creating Interactive Quizzes:**\n",
      "    You can create quizzes using Blade templates and JavaScript. Laravel uses Vue.js by default, but you can use another library if you prefer. You'll need to store quiz questions in a database table and then fetch them when the user starts the quiz. \n",
      "\n",
      "11. **Handling Quiz Answers:**\n",
      "    Store user's quiz answers in a database table. You can then fetch these answers when you need to display quiz results.\n",
      "\n",
      "12. **Setting up User Authentication:**\n",
      "    Laravel provides an authentication scaffolding, which sets up user registration, authentication, and password recovery. You can use `php artisan make:command MakeAuthCommand` to make these features.\n",
      "\n",
      "13. **Error Handling and Testing:**\n",
      "    Laravel provides several ways for error handling, like middleware and form requests. Always remember to test your application. Laravel provides a helper function ` страartest` to start testing.\n",
      "\n",
      "Creating an e-learning platform is a complex task and requires a good understanding of PHP and Laravel. It's also important to have a good understanding of JavaScript, HTML, and CSS for the front-end part. Don't forget to make your platform user-friendly and engaging by using best practices in UX/UI design.\n",
      "\n",
      "Also, remember that creating an e-learning platform involves several legal and ethical considerations, including data privacy and intellectual property rights. Make sure to comply with all applicable laws and regulations.</s>\n",
      "\n",
      "Rejected sample 74 of the raw training set:\n",
      "\n",
      "Here's a suggested high-level overview of the development workflow for building an e-learning platform with multimedia content and interactive quizzes using the Laravel framework:\n",
      "\n",
      "1. Planning and requirements gathering:\n",
      "\n",
      "\tDefine the scope of the project, including the features, functionality, and requirements for the e-learning platform. This will help you prioritize and plan the development stages.\n",
      "\n",
      "\tDetermine the technology stack, including the programming language, framework, and any libraries or third-party tools that will be used.\n",
      "\n",
      "\tDecide on the database structure for the platform, including the entities and relationships between them.\n",
      "\n",
      "\tPlan the user authentication and authorization process, including the registration and login flows, and the roles and permissions required for different user groups.\n",
      "\n",
      "2. Development:\n",
      "\n",
      "\tImplement the backend functionality, including the courses, multimedia content, and quizzes.\n",
      "\n",
      "\tImplement the user authentication and authorization system.\n",
      "\n",
      "\tImplement the frontend website, including the user dashboard, course pages, and quiz pages.\n",
      "\n",
      "\tIntegrate the multimedia content storage and streaming service, such as Amazon S3 or YouTube.\n",
      "\n",
      "\tImplement the quizzes, including the question bank, question selection, and scoring.\n",
      "\n",
      "3. Testing:\n",
      "\n",
      "\tWrite unit tests for the backend functionality, including the courses, multimedia content, and quizzes.\n",
      "\n",
      "\tWrite integration tests to test the backend interactions with the database and other systems.\n",
      "\n",
      "\tCreate automated tests for the frontend website, including UI tests and end-to-end tests.\n",
      "\n",
      "\tCreate user acceptance tests to test the platform functionality from the user's perspective.\n",
      "\n",
      "4. Deployment:\n",
      "\n",
      "\tPrepare the platform for deployment, including configuration, security, and performance tuning.\n",
      "\n",
      "\tDeploy the platform to the production environment.\n",
      "\n",
      "\tImplement continuous integration and continuous deployment (CI/CD) pipelines to automate the deployment process.\n",
      "\n",
      "5. Maintenance and support:\n",
      "\n",
      "\tContinue to maintain and support the e-learning platform, including bug fixes, enhancements, and feature updates.\n",
      "\n",
      "Note: This is a general overview and the specific stages and details may vary depending on the project scope and requirements. It's always a good idea to break the development process down into smaller, manageable stages, with clear milestones and deliverables, for a more efficient and effective workflow.</s>\n",
      "\n",
      "Prompt sample 92 of the raw training set:\n",
      "\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "How can I use Go to simulate the impact of a new agricultural pest on crop yields in the next decade? Specifically, how can I model its spread patterns and evaluate its resistance to current control methods? Additionally, how can I estimate the potential economic losses for farmers and identify strategies to mitigate them? Please provide code examples if possible.\n",
      "Assuming the agricultural pest data is available in JSON format, how can I use Go to extract relevant information and perform the necessary simulations and evaluations? Can you provide an example of how to parse and manipulate the JSON data in Go?</s>\n",
      "<|assistant|>\n",
      "\n",
      "Chosen sample 92 of the raw training set:\n",
      "\n",
      "Certainly! To perform all these tasks in Go, you can break them down into smaller steps and tackle them one by one. The basic steps would be: read the input JSON data, create a model to simulate the pest's impact and control, and perform calculations to estimate economic losses.\n",
      "\n",
      "Let's start with parsing the JSON data. First, install `encoding/json` package if you haven't:\n",
      "\n",
      "```go\n",
      "import \"encoding/json\"\n",
      "```\n",
      "\n",
      "Assuming you have a JSON structure like this:\n",
      "\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"name\": \"Pest A\",\n",
      "    \"growth_rate\": 1.25,\n",
      "    \"resistance\": 0.75,\n",
      "    \"initial_population\": 1000\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "You would define a `Pest` struct in Go:\n",
      "\n",
      "```go\n",
      "type Pest struct {\n",
      "\tName            string  `json:\"name\"`\n",
      "\tGrowthRate      float64 `json:\"growth_rate\"`\n",
      "\tResistance      float64 `json:\"resistance\"`\n",
      "\tInitialPopulation int    `json:\"initial_population\"`\n",
      "}\n",
      "```\n",
      "\n",
      "And then, you could parse the JSON data and unmarshal it into a slice of `Pest`:\n",
      "\n",
      "```go\n",
      "func main() {\n",
      "\tjsonData := []byte(`[\n",
      "      {\n",
      "        \"name\": \"Pest A\",\n",
      "        \"growth_rate\": 1.25,\n",
      "        \"resistance\": 0.75,\n",
      "        \"initial_population\": 1000\n",
      "      }\n",
      "    ]`)\n",
      "\n",
      "\tvar pests []Pest\n",
      "\terr := json.Unmarshal(jsonData, &pests)\n",
      "\tif err != nil {\n",
      "\t\tfmt.Println(err)\n",
      "\t\treturn\n",
      "\t}\n",
      "\n",
      "\t// Perform calculations and simulations\n",
      "\t// ...\n",
      "}\n",
      "```\n",
      "\n",
      "To simulate the impact of a pest on crop yields, you need to develop a model that incorporates the multiple factors: pest growth rate, control method effectiveness, economic impact, etc.\n",
      "\n",
      "Let's define a simple simulation function that calculates the total pest population after a certain number of years:\n",
      "\n",
      "```go\n",
      "func simulatePestSpread(pest Pest, years int, controlEffectiveness float64) int {\n",
      "\tpopulation := float64(pest.InitialPopulation)\n",
      "\tfor i := 0; i < years; i++ {\n",
      "\t\tpopulation *= pest.GrowthRate\n",
      "\t\tpopulation *= (1 - controlEffectiveness*pest.Resistance)\n",
      "\t}\n",
      "\treturn int(population)\n",
      "}\n",
      "```\n",
      "\n",
      "Now, assuming you have data on the impact of pest population on crop yield, you can create a function to calculate the economic losses for farmers:\n",
      "\n",
      "```go\n",
      "func estimateEconomicLoss(pestPopulation int, impactData map[int]float64) float64 {\n",
      "\tloss := 0.0\n",
      "\tfor population, impact := range impactData {\n",
      "\t\tif pestPopulation >= population {\n",
      "\t\t\tloss = impact\n",
      "\t\t} else {\n",
      "\t\t\tbreak\n",
      "\t\t}\n",
      "\t}\n",
      "\treturn loss\n",
      "}\n",
      "```\n",
      "\n",
      "Finally, you can create a function that evaluates different control strategies and chooses the one that minimizes the economic losses:\n",
      "\n",
      "```go\n",
      "func findBestControlStrategy(pest Pest, years int, strategies []float64, impactData map[int]float64) (float64, float64) {\n",
      "\tminLoss := math.Inf(1)\n",
      "\tbestStrategy := 0.0\n",
      "\n",
      "\tfor _, strategy := range strategies {\n",
      "\t\tpopulation := simulatePestSpread(pest, years, strategy)\n",
      "\t\tloss := estimateEconomicLoss(population, impactData)\n",
      "\t\tif loss < minLoss {\n",
      "\t\t\tminLoss = loss\n",
      "\t\t\tbestStrategy = strategy\n",
      "\t\t}\n",
      "\t}\n",
      "\n",
      "\treturn bestStrategy, minLoss\n",
      "}\n",
      "```\n",
      "\n",
      "Now you can use these functions to simulate the impact of a given pest under various control strategies and find the best one. Note that this example is quite simple and may not accurately represent the complexity of real-life pest spread and control strategies. You should adapt and extend the model according to the domain knowledge and data you have.\n",
      "\n",
      "You can find the fully working code snippet in this GitHub Gist: https://gist.github.com/enzonaut/1ef40a8e3871b99cb1569dbec2a5bb2c</s>\n",
      "\n",
      "Rejected sample 92 of the raw training set:\n",
      "\n",
      "As an AI assistant, I'll be happy to provide you with an overview of how you can use Go to simulate the impact of a new agricultural pest on crop yields in the next decade. This includes modeling the pest's spread patterns, evaluating its resistance to current control methods, and estimating potential economic losses for farmers, as well as identifying strategies for mitigating them.\n",
      "\n",
      "1. First, let's consider how to parse the JSON data. You can use Go's built-in `encoding/json` package to parse JSON data. Here's an example code snippet that demonstrates how to do that:\n",
      "```go\n",
      "import (\n",
      "    \"encoding/json\"\n",
      "    \"fmt\"\n",
      "    \"strings\"\n",
      ")\n",
      "\n",
      "type PestData struct {\n",
      "    Name        string `json:\"name\"`\n",
      "    ScientificName string `json:\"scientific_name\"`\n",
      "    Origin       string `json:\"origin\"`\n",
      "    SpreadPattern string `json:\"spread_pattern\"`\n",
      "}\n",
      "\n",
      "func main() {\n",
      "    // Assuming you have the JSON data as a string\n",
      "    jsonData := `{\"name\": \"NewPest\", \"scientific_name\": \"AgriculturalPest\", \"origin\": \"Asia\", \"spread_pattern\": \"features latent period and high reproductive rate\"}`\n",
      "\n",
      "    var pestData PestData\n",
      "    err := json.Unmarshal([]byte(jsonData), &pestData)\n",
      "    if err != nil {\n",
      "        panic(err)\n",
      "    }\n",
      "\n",
      "    fmt.Printf(\"Name: %s\\n\", pestData.Name)\n",
      "    fmt.Printf(\"Scientific Name: %s\\n\", pestData.ScientificName)\n",
      "    fmt.Printf(\"Origin: %s\\n\", pestData.Origin)\n",
      "    fmt.Printf(\"Spread Pattern: %s\\n\", pestData.SpreadPattern)\n",
      "}\n",
      "```\n",
      "In this example, we define a `PestData` struct that matches the JSON schema, and then use Go's `json.Unmarshal()` function to parse the JSON data into this struct. Finally, we print out the values of the struct's fields.\n",
      "\n",
      "1. Next, let's discuss how to model the pest's spread patterns. You can use Go's built-in `math/rand` package to simulate random movements of the pest. Here's an example code snippet that demonstrates how to simulated the pest's spread in a 2D grid:\n",
      "```go\n",
      "func simulateSpread(grid [][]int, pestData PestData) [][]int {\n",
      "    maxPestDensity := (len(grid) * len(grid[0])) * pestData.SpreadPattern\n",
      "    if maxPestDensity > len(grid) {\n",
      "        maxPestDensity = len(grid)\n",
      "    }\n",
      "    rand.Seed(time.Now().UnixNano())\n",
      "    for i := 0; i < maxPestDensity; i++ {\n",
      "        row := rand.Intn(len(grid))\n",
      "        col := rand.Intn(len(grid[0]))\n",
      "        if grid[row][col] == 0 {\n",
      "            grid[row][col] = 1\n",
      "        }\n",
      "    }\n",
      "    return grid\n",
      "}\n",
      "\n",
      "func main() {\n",
      "    // Generate your 2D grid\n",
      "    grid := make([][]int, 10)\n",
      "    for i := 0; i < 10; i++ {\n",
      "        row := make([]int, 10)\n",
      "        for j := 0; j < 10; j++ {\n",
      "            row[j] = j + 1\n",
      "        }\n",
      "        grid[i] = row\n",
      "    }\n",
      "\n",
      "    // Simulate the pest's spread\n",
      "    pestData.SpreadPattern = \"features latent period and high reproductive rate\"\n",
      "    simulatedGrid := simulateSpread(grid, pestData)\n",
      "\n",
      "    // Your code for evaluating resistance to control methods and estimating economic losses goes here\n",
      "}\n",
      "```\n",
      "In this example, we define a `simulateSpread()` function that takes a grid and a pestData struct as inputs, simulates the pest'</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Print a few random samples from the training set:\n",
    "for index in random.sample(range(len(raw_datasets[\"train\"])), 3):\n",
    "    print(f\"Prompt sample {index} of the raw training set:\\n\\n{raw_datasets['train'][index]['prompt']}\")\n",
    "    print(f\"Chosen sample {index} of the raw training set:\\n\\n{raw_datasets['train'][index]['chosen']}\")\n",
    "    print(f\"Rejected sample {index} of the raw training set:\\n\\n{raw_datasets['train'][index]['rejected']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44ac264-1ac2-431f-a215-5f2f66ab47c8",
   "metadata": {},
   "source": [
    "### SFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75fa8c3b-9a1c-40dd-8bd8-32a118d30e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter weights model repo: alignment-handbook/zephyr-7b-sft-lora\n",
      "Base model weights model repo: mistralai/Mistral-7B-v0.1\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftConfig\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(model_id)\n",
    "print(\"Adapter weights model repo:\", model_id)\n",
    "print(\"Base model weights model repo:\", peft_config.base_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cba07ee-ce0f-4d0d-abe1-de5d20e86dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a406169-925b-4e44-b06d-fc5ef5c231f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69781ae3-84a8-47fd-a464-e11b00af99fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify how to quantize the model\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e18efb23-04a3-4644-9499-523e623d5441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6069d78e96a141e68501eb0bc13aefaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: load the base model (Mistral-7B in our case) in 4-bit\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"flash_attention_2\", # set this to True if your GPU supports it (Flash Attention drastically speeds up model computations)\n",
    "    torch_dtype=\"auto\",\n",
    "    use_cache=False,  # set to False as we're going to use gradient checkpointing\n",
    "    device_map=device_map,\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(peft_config.base_model_name_or_path, **model_kwargs)\n",
    "\n",
    "# Step 2: load base model + SFT adapter weights\n",
    "# notice that only the adapter weights are trainable!\n",
    "model = PeftModel.from_pretrained(base_model, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6acf2f2-9898-43f8-aa31-216aa363957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf10cb2d-42c1-493a-a38b-acfec3f7f046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_trainable_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02baee0e-bac1-4c03-9f81-497fdf5a4a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3eb12fef-1514-42dc-b826-54ef7b48ec1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 3794014208 || trainable%: 0.0\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19502ab-56d6-4644-9e1c-c1ed6871aa77",
   "metadata": {},
   "source": [
    "### DPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ef1140b-7849-43a7-9e92-19afd6eea0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['NCCL_P2P_DISABLE'] = '1' \n",
    "os.environ['NCCL_IB_DISABLE'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4f45d83-6933-4b51-a819-87b5cce1c407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPOTrainer??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82526a20-4afc-4e9c-b932-32fa57f55dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whaow/anaconda3/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.0.1 with CUDA 1108 (you have 2.2.2+cu121)\n",
      "    Python  3.10.13 (you have 3.10.13)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-28 00:41:51,923] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whaow/anaconda3/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:314: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "from trl import DPOTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# path where the Trainer will save its checkpoints and logs\n",
    "output_dir = 'data/zephyr-7b-dpo-lora'\n",
    "\n",
    "# based on config\n",
    "training_args = TrainingArguments(\n",
    "    bf16=True,\n",
    "    # beta=0.01,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\":False},\n",
    "    hub_model_id=\"zephyr-7b-dpo-qlora\",\n",
    "    learning_rate=5.0e-6,\n",
    "    log_level=\"info\",\n",
    "    logging_steps=10,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    # max_length=1024,\n",
    "    # max_prompt_length=512,\n",
    "    num_train_epochs=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    output_dir=output_dir,  # It is handy to append `hub_model_revision` to keep track of your local experiments\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=8,\n",
    "    # push_to_hub=True,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    warmup_ratio=0.1,\n",
    ")\n",
    "\n",
    "# based on the recipe: https://github.com/huggingface/alignment-handbook/blob/main/recipes/zephyr-7b-beta/dpo/config_qlora.yaml\n",
    "peft_config = LoraConfig(\n",
    "        r=128,\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\",  \"up_proj\",  \"down_proj\"],\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "        model,\n",
    "        ref_model=None,\n",
    "        model_init_kwargs=None,\n",
    "        ref_model_init_kwargs=None,\n",
    "        args=training_args,\n",
    "        # beta=training_args.beta,\n",
    "        train_dataset=raw_datasets[\"train\"],\n",
    "        eval_dataset=raw_datasets[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=1024,\n",
    "        max_prompt_length=512,\n",
    "        peft_config=peft_config,\n",
    "        # loss_type=training_args.loss_type,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "582006bd-0057-4e97-b600-e4cdbe432fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using 8-bit optimizers with a version of `bitsandbytes` < 0.41.1. It is recommended to update your version as a major bug has been fixed in 8-bit optimizers.\n",
      "***** Running training *****\n",
      "  Num examples = 100\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Training with DataParallel so batch size has been adjusted to: 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 12\n",
      "  Number of trainable parameters = 335,544,320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlanchunhui\u001b[0m (\u001b[33mloveresearch\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b957497ee40845ba884bd8109d63f7b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112515810721865, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/whaow/workspaces/personal_chatgpt/tutorials/trl_hf/wandb/run-20240528_004202-bx08za43</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/loveresearch/huggingface/runs/bx08za43' target=\"_blank\">rose-silence-66</a></strong> to <a href='https://wandb.ai/loveresearch/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/loveresearch/huggingface' target=\"_blank\">https://wandb.ai/loveresearch/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/loveresearch/huggingface/runs/bx08za43' target=\"_blank\">https://wandb.ai/loveresearch/huggingface/runs/bx08za43</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3/12 00:09 < 01:27, 0.10 it/s, Epoch 0.16/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 435.69 MiB is free. Including non-PyTorch memory, this process has 23.20 GiB memory in use. Of the allocated memory 20.84 GiB is allocated by PyTorch, and 1.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspaces/learning/transformers/src/transformers/trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1541\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspaces/learning/transformers/src/transformers/trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1860\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1865\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/workspaces/learning/transformers/src/transformers/trainer.py:2737\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2736\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2737\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2740\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:1055\u001b[0m, in \u001b[0;36mDPOTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1052\u001b[0m compute_loss_context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_peft_has_been_casted_to_bf16 \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compute_loss_context_manager():\n\u001b[0;32m-> 1055\u001b[0m     loss, metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_loss_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# force log the metrics\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_metrics(metrics, train_eval\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:996\u001b[0m, in \u001b[0;36mDPOTrainer.get_batch_loss_metrics\u001b[0;34m(self, model, batch, train_eval)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the DPO loss and other metrics for the given batch of inputs for train or test.\"\"\"\u001b[39;00m\n\u001b[1;32m    989\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    991\u001b[0m (\n\u001b[1;32m    992\u001b[0m     policy_chosen_logps,\n\u001b[1;32m    993\u001b[0m     policy_rejected_logps,\n\u001b[1;32m    994\u001b[0m     policy_chosen_logits,\n\u001b[1;32m    995\u001b[0m     policy_rejected_logits,\n\u001b[0;32m--> 996\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenated_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;66;03m# if reference_chosen_logps and reference_rejected_logps in batch use them, otherwise use the reference model\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference_chosen_logps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference_rejected_logps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:966\u001b[0m, in \u001b[0;36mDPOTrainer.concatenated_forward\u001b[0;34m(self, model, batch)\u001b[0m\n\u001b[1;32m    952\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    953\u001b[0m     {\n\u001b[1;32m    954\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: concatenated_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcatenated_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    959\u001b[0m )\n\u001b[1;32m    960\u001b[0m all_logits \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m    961\u001b[0m     concatenated_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcatenated_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    962\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mconcatenated_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcatenated_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    964\u001b[0m )\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m--> 966\u001b[0m all_logps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_logps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcatenated_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconcatenated_labels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43maverage_log_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_encoder_decoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_encoder_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_pad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_pad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m chosen_logps \u001b[38;5;241m=\u001b[39m all_logps[:len_chosen]\n\u001b[1;32m    975\u001b[0m rejected_logps \u001b[38;5;241m=\u001b[39m all_logps[len_chosen:]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:929\u001b[0m, in \u001b[0;36mDPOTrainer.get_batch_logps\u001b[0;34m(logits, labels, average_log_prob, label_pad_token_id, is_encoder_decoder)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# dummy token; we'll ignore the losses on these tokens later\u001b[39;00m\n\u001b[1;32m    927\u001b[0m labels[labels \u001b[38;5;241m==\u001b[39m label_pad_token_id] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 929\u001b[0m per_token_logps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mgather(\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, index\u001b[38;5;241m=\u001b[39mlabels\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average_log_prob:\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (per_token_logps \u001b[38;5;241m*\u001b[39m loss_mask)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m loss_mask\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 435.69 MiB is free. Including non-PyTorch memory, this process has 23.20 GiB memory in use. Of the allocated memory 20.84 GiB is allocated by PyTorch, and 1.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e56511d-93aa-48ca-94cc-870035042300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
