{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa570992-7300-4e41-a673-f54a828b562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a575267-4a0c-47ee-8fdc-a53533f4a0e4",
   "metadata": {},
   "source": [
    "- misc\n",
    "    - https://github.com/huggingface/alignment-handbook/tree/main\n",
    "- 3 steps\n",
    "    - pre-training a large language model (LLM) to predict the next token on internet-scale data, on clusters of thousands of GPUs. One calls the result a **\"base model\"**\n",
    "    - supervised fine-tuning (SFT) to turn the base model into a useful assistant (ChatBot)\n",
    "        - we turned a \"base model\" into a useful assistant, by training it to **generate useful completions given human instructions.**\n",
    "    - human preference fine-tuning which increases the assistant's friendliness, helpfulness and safety.\n",
    "        - \"safe\", \"friendly\", \"harmless\", \"inclusive\",\n",
    "        - human preference fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74ea9ae-cabd-46d4-8041-10d0d5c7f768",
   "metadata": {},
   "source": [
    "## align & why align"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8139c761-3081-4fa0-8c22-06d7f4867863",
   "metadata": {},
   "source": [
    "- dpo: direct preference optimization your language model is **secretly a reward model**\n",
    "    - https://arxiv.org/abs/2305.18290\n",
    "- collect human/ai feedback to learn $p(y_w\\gt y_l)$\n",
    "- RLHF - the OG（Original Gangster，始祖） of LLM alignment\n",
    "\n",
    "    $$\n",
    "    \\max_{\\pi_\\theta} \\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim \\pi_\\theta(y \\mid x)} \\underbrace{\\left[ r_\\phi(x, y) \\right]}_{\\text{maximise rewards}} - \\underbrace{\\beta \\mathbb{D}_{\\text{KL}} \\left[ \\pi_\\theta(y \\mid x) \\parallel \\pi_{\\text{ref}}(y \\mid x) \\right]}_{\\text{use KL penalty to prevent\n",
    "    reward hacking (controlled by β)\n",
    "    }}\n",
    "    $$\n",
    "    - RL（PPO）很多超参，且训练不稳定；\n",
    "    - 还需要一个RM（Reward model），这样的话一共三个model要操作，actor model，ref model，Reward model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a086b96-47b3-4f68-b6b0-daae6f0f4665",
   "metadata": {},
   "source": [
    "## DPO（Direct Preference Optimization）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff27b752-39a3-4887-b7eb-6616e1257fbc",
   "metadata": {},
   "source": [
    "$$\n",
    "\\max_{\\pi} \\mathbb{E}_{(x, y_w, y_l) \\sim D} \\log \\sigma \\left( \\beta \\log \\frac{\\pi(y_w | x)}{\\pi_{\\text{ref}}(y_w | x)} - \\beta \\log \\frac{\\pi(y_l | x)}{\\pi_{\\text{ref}}(y_l | x)} \\right)\n",
    "$$\n",
    "- only two models (actor/active model, reference model (sft))\n",
    "- 求导练习\n",
    "\n",
    "    $$\n",
    "    \\left(\\log\\sigma(z)\\right))'=\\frac{1}{\\sigma(z)}\\cdot \\sigma(z)(1-\\sigma(z))=1-\\sigma(z)=\\sigma(-z)\n",
    "    $$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta} \\mathcal{L}_{\\text{DPO}} (\\pi_{\\theta}; \\pi_{\\text{ref}}) = & -\\beta \\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\underbrace{\\sigma \\left( \\hat{r}_{\\theta}(x, y_l) - \\hat{r}_{\\theta}(x, y_w) \\right)}_{\\text{higher weight when reward estimate is wrong} } \\left[ \\underbrace{\\nabla_{\\theta} \\log \\pi(y_w | x)}_{\\text{increase likelihood of } y_w} - \\underbrace{\\nabla_{\\theta} \\log \\pi(y_l | x)}_{\\text{decrease likelihood of } y_l} \\right] \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- $\\hat r_\\theta(x,y)=\\beta\\log\\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}$（implicit reward from LM）\n",
    "    - 它表示的是模型 $\\pi_\\theta$ 相对于参考模型 $\\pi_{\\text{ref}}$ 对生成结果 $y$ 的偏好程度。\n",
    "    - 与显式奖励（例如通过人工评分或者明确的奖励函数给出的奖励）不同，隐式奖励是通过模型内部的概率分布计算得到的。在DPO中，这种隐式奖励直接来源于模型本身的输出概率分布，因此称为“隐式奖励”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf81006e-6215-480f-9a76-235ed38ae224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F487af2f0-e51d-4140-92a7-23476c5ea016_1600x1015.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F487af2f0-e51d-4140-92a7-23476c5ea016_1600x1015.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1869cb8-02bf-4671-8f7e-53dcd31af57b",
   "metadata": {},
   "source": [
    "## practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2cbb16-2856-47d0-a274-705588b09825",
   "metadata": {},
   "source": [
    "- https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama_2/scripts/\n",
    "    - dpo_llama2.py\n",
    "\n",
    "```\n",
    "accelerate launch examples/research_projects/stack_llama_2/scripts/dpo_llama2.py \\\n",
    "    --model_name_or_path=\"sft/final_checkpoint\" \\\n",
    "    --output_dir=\"dpo\"\n",
    "```\n",
    "\n",
    "- basemodel: `meta-llama/Llama-2-7b-hf`\n",
    "- sft:\n",
    "- dpo (alignment => rlhf):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd346a9-26e0-4e6f-a96a-8f785a238b0d",
   "metadata": {},
   "source": [
    "- dataset\n",
    "    - lvwerra/stack-exchange-paired\n",
    "        - question, Create pairs (response_j, response_k) where j was rated better than k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
