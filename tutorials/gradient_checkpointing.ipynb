{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b19b81b9",
   "metadata": {},
   "source": [
    "## basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e5c2aa",
   "metadata": {},
   "source": [
    "- https://arxiv.org/pdf/1604.06174.pdf\n",
    "- https://medium.com/tensorflow/fitting-larger-networks-into-memory-\n",
    "    - **computation graph examples**\n",
    "    - 节点在必要时才加载进来\n",
    "    - At the peak, the algorithm stores all activations,\n",
    "        - forward\n",
    "        - $O(n)$ memory requirement for network of depth $n$.\n",
    "    - CHECKPOINT: recomputing them later.\n",
    "        - More generally, this “memory-poor” strategy needs $O(1)$ memory but requires $O(n^2)$ computation steps.\n",
    "- https://huggingface.co/docs/transformers/v4.18.0/en/performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f3dfc",
   "metadata": {},
   "source": [
    "- 依然是显存占用优化算法\n",
    "    - 当然是 memory usage 与 computation time 之间的 tradeoff 在反向传播过程中；\n",
    "- In deep neural networks, backpropagation requires storing **intermediate activations** for computing gradients during the backward pass. \n",
    "    - 但是当层数变多时，存储所有的中间层的激活值（intermediate activations）非常地占用显存；\n",
    "- gradient checkpointing 选择性地重新计算（recompute）一部分的 intermediate activations 在反向传播过程中来缓解显存的压力；\n",
    "    - Instead of storing all activations (**during the forward pass**), only a **subset** of them, typically those necessary for computing gradients, are cached. \n",
    "    - The remaining intermediate activations are recomputed on-the-fly **during the backward pass**. By recomputing rather than storing all intermediate activations, memory usage is reduced at the cost of increased computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ed43a1",
   "metadata": {},
   "source": [
    "## Trainer Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d124e5aa",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f95f27b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T15:04:55.013134Z",
     "start_time": "2023-07-31T15:04:53.609057Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "seq_len, dataset_size = 512, 512\n",
    "dummy_data = {\n",
    "    \"input_ids\": np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
    "    \"labels\": np.random.randint(0, 2, (dataset_size)),\n",
    "}\n",
    "ds = Dataset.from_dict(dummy_data)\n",
    "ds.set_format(\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b6f0be9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T14:53:13.845866Z",
     "start_time": "2023-07-31T14:53:13.838760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512) (512,)\n"
     ]
    }
   ],
   "source": [
    "print(dummy_data['input_ids'].shape, dummy_data['labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2701b096",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T15:19:32.718495Z",
     "start_time": "2023-07-31T15:19:32.708626Z"
    }
   },
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28f6432c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T14:53:54.864782Z",
     "start_time": "2023-07-31T14:53:54.838756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 352 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c195f2aa",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "802392c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T15:04:58.822052Z",
     "start_time": "2023-07-31T15:04:58.686578Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "959709b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T15:05:10.360309Z",
     "start_time": "2023-07-31T15:05:00.328343Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('bert-large-uncased').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9310804c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T14:55:36.306701Z",
     "start_time": "2023-07-31T14:55:36.298668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 2511 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2289fcb3",
   "metadata": {},
   "source": [
    "### training without checkpint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5617c8c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T15:05:33.621730Z",
     "start_time": "2023-07-31T15:05:33.307141Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, logging\n",
    "\n",
    "default_args = {\n",
    "    \"output_dir\": \"tmp\",\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"log_level\": \"error\",\n",
    "    \"report_to\": \"none\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afc31b97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T14:56:39.810674Z",
     "start_time": "2023-07-31T14:56:16.807027Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whaow/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 22.528, 'train_samples_per_second': 22.727, 'train_steps_per_second': 2.841, 'train_loss': 0.7311427593231201, 'epoch': 1.0}\n",
      "Time: 22.53\n",
      "Samples/second: 22.73\n",
      "GPU memory occupied: 12507 MB.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ca94bd",
   "metadata": {},
   "source": [
    "### training with checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bafc5e2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T15:19:14.749947Z",
     "start_time": "2023-07-31T15:05:35.362143Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whaow/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 13:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'print_summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model\u001b[38;5;241m=\u001b[39mmodel, args\u001b[38;5;241m=\u001b[39mtraining_args, train_dataset\u001b[38;5;241m=\u001b[39mds)\n\u001b[1;32m      6\u001b[0m result \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 7\u001b[0m \u001b[43mprint_summary\u001b[49m(result)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'print_summary' is not defined"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1, gradient_accumulation_steps=4, gradient_checkpointing=True, **default_args\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds)\n",
    "result = trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "748e4407",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T15:19:41.118936Z",
     "start_time": "2023-07-31T15:19:41.111203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 819.17\n",
      "Samples/second: 0.62\n",
      "GPU memory occupied: 12865 MB.\n"
     ]
    }
   ],
   "source": [
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f525df59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T15:19:48.831031Z",
     "start_time": "2023-07-31T15:19:48.820289Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# steps(total batches)\n",
    "512/((1*2) * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d00cbd44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T15:20:56.225209Z",
     "start_time": "2023-07-31T15:20:56.204904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight cuda:0\n",
      "bert.embeddings.position_embeddings.weight cuda:0\n",
      "bert.embeddings.token_type_embeddings.weight cuda:0\n",
      "bert.embeddings.LayerNorm.weight cuda:0\n",
      "bert.embeddings.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.0.attention.self.query.weight cuda:0\n",
      "bert.encoder.layer.0.attention.self.query.bias cuda:0\n",
      "bert.encoder.layer.0.attention.self.key.weight cuda:0\n",
      "bert.encoder.layer.0.attention.self.key.bias cuda:0\n",
      "bert.encoder.layer.0.attention.self.value.weight cuda:0\n",
      "bert.encoder.layer.0.attention.self.value.bias cuda:0\n",
      "bert.encoder.layer.0.attention.output.dense.weight cuda:0\n",
      "bert.encoder.layer.0.attention.output.dense.bias cuda:0\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.0.intermediate.dense.weight cuda:0\n",
      "bert.encoder.layer.0.intermediate.dense.bias cuda:0\n",
      "bert.encoder.layer.0.output.dense.weight cuda:0\n",
      "bert.encoder.layer.0.output.dense.bias cuda:0\n",
      "bert.encoder.layer.0.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.0.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.1.attention.self.query.weight cuda:0\n",
      "bert.encoder.layer.1.attention.self.query.bias cuda:0\n",
      "bert.encoder.layer.1.attention.self.key.weight cuda:0\n",
      "bert.encoder.layer.1.attention.self.key.bias cuda:0\n",
      "bert.encoder.layer.1.attention.self.value.weight cuda:0\n",
      "bert.encoder.layer.1.attention.self.value.bias cuda:0\n",
      "bert.encoder.layer.1.attention.output.dense.weight cuda:0\n",
      "bert.encoder.layer.1.attention.output.dense.bias cuda:0\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.1.intermediate.dense.weight cuda:0\n",
      "bert.encoder.layer.1.intermediate.dense.bias cuda:0\n",
      "bert.encoder.layer.1.output.dense.weight cuda:0\n",
      "bert.encoder.layer.1.output.dense.bias cuda:0\n",
      "bert.encoder.layer.1.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.1.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.2.attention.self.query.weight cuda:0\n",
      "bert.encoder.layer.2.attention.self.query.bias cuda:0\n",
      "bert.encoder.layer.2.attention.self.key.weight cuda:0\n",
      "bert.encoder.layer.2.attention.self.key.bias cuda:0\n",
      "bert.encoder.layer.2.attention.self.value.weight cuda:0\n",
      "bert.encoder.layer.2.attention.self.value.bias cuda:0\n",
      "bert.encoder.layer.2.attention.output.dense.weight cuda:0\n",
      "bert.encoder.layer.2.attention.output.dense.bias cuda:0\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.2.intermediate.dense.weight cuda:0\n",
      "bert.encoder.layer.2.intermediate.dense.bias cuda:0\n",
      "bert.encoder.layer.2.output.dense.weight cuda:0\n",
      "bert.encoder.layer.2.output.dense.bias cuda:0\n",
      "bert.encoder.layer.2.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.2.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.3.attention.self.query.weight cuda:0\n",
      "bert.encoder.layer.3.attention.self.query.bias cuda:0\n",
      "bert.encoder.layer.3.attention.self.key.weight cuda:0\n",
      "bert.encoder.layer.3.attention.self.key.bias cuda:0\n",
      "bert.encoder.layer.3.attention.self.value.weight cuda:0\n",
      "bert.encoder.layer.3.attention.self.value.bias cuda:0\n",
      "bert.encoder.layer.3.attention.output.dense.weight cuda:0\n",
      "bert.encoder.layer.3.attention.output.dense.bias cuda:0\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.3.intermediate.dense.weight cuda:0\n",
      "bert.encoder.layer.3.intermediate.dense.bias cuda:0\n",
      "bert.encoder.layer.3.output.dense.weight cuda:0\n",
      "bert.encoder.layer.3.output.dense.bias cuda:0\n",
      "bert.encoder.layer.3.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.3.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.4.attention.self.query.weight cuda:0\n",
      "bert.encoder.layer.4.attention.self.query.bias cuda:0\n",
      "bert.encoder.layer.4.attention.self.key.weight cuda:0\n",
      "bert.encoder.layer.4.attention.self.key.bias cuda:0\n",
      "bert.encoder.layer.4.attention.self.value.weight cuda:0\n",
      "bert.encoder.layer.4.attention.self.value.bias cuda:0\n",
      "bert.encoder.layer.4.attention.output.dense.weight cuda:0\n",
      "bert.encoder.layer.4.attention.output.dense.bias cuda:0\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.4.intermediate.dense.weight cuda:0\n",
      "bert.encoder.layer.4.intermediate.dense.bias cuda:0\n",
      "bert.encoder.layer.4.output.dense.weight cuda:0\n",
      "bert.encoder.layer.4.output.dense.bias cuda:0\n",
      "bert.encoder.layer.4.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.4.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.5.attention.self.query.weight cuda:0\n",
      "bert.encoder.layer.5.attention.self.query.bias cuda:0\n",
      "bert.encoder.layer.5.attention.self.key.weight cuda:0\n",
      "bert.encoder.layer.5.attention.self.key.bias cuda:0\n",
      "bert.encoder.layer.5.attention.self.value.weight cuda:0\n",
      "bert.encoder.layer.5.attention.self.value.bias cuda:0\n",
      "bert.encoder.layer.5.attention.output.dense.weight cuda:0\n",
      "bert.encoder.layer.5.attention.output.dense.bias cuda:0\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.5.intermediate.dense.weight cuda:0\n",
      "bert.encoder.layer.5.intermediate.dense.bias cuda:0\n",
      "bert.encoder.layer.5.output.dense.weight cuda:0\n",
      "bert.encoder.layer.5.output.dense.bias cuda:0\n",
      "bert.encoder.layer.5.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.5.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.6.attention.self.query.weight cuda:0\n",
      "bert.encoder.layer.6.attention.self.query.bias cuda:0\n",
      "bert.encoder.layer.6.attention.self.key.weight cuda:0\n",
      "bert.encoder.layer.6.attention.self.key.bias cuda:0\n",
      "bert.encoder.layer.6.attention.self.value.weight cuda:0\n",
      "bert.encoder.layer.6.attention.self.value.bias cuda:0\n",
      "bert.encoder.layer.6.attention.output.dense.weight cuda:0\n",
      "bert.encoder.layer.6.attention.output.dense.bias cuda:0\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.6.intermediate.dense.weight cuda:0\n",
      "bert.encoder.layer.6.intermediate.dense.bias cuda:0\n",
      "bert.encoder.layer.6.output.dense.weight cuda:0\n",
      "bert.encoder.layer.6.output.dense.bias cuda:0\n",
      "bert.encoder.layer.6.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.6.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.7.attention.self.query.weight cuda:0\n",
      "bert.encoder.layer.7.attention.self.query.bias cuda:0\n",
      "bert.encoder.layer.7.attention.self.key.weight cuda:0\n",
      "bert.encoder.layer.7.attention.self.key.bias cuda:0\n",
      "bert.encoder.layer.7.attention.self.value.weight cuda:0\n",
      "bert.encoder.layer.7.attention.self.value.bias cuda:0\n",
      "bert.encoder.layer.7.attention.output.dense.weight cuda:0\n",
      "bert.encoder.layer.7.attention.output.dense.bias cuda:0\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.7.intermediate.dense.weight cuda:0\n",
      "bert.encoder.layer.7.intermediate.dense.bias cuda:0\n",
      "bert.encoder.layer.7.output.dense.weight cuda:0\n",
      "bert.encoder.layer.7.output.dense.bias cuda:0\n",
      "bert.encoder.layer.7.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.7.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.8.attention.self.query.weight cuda:0\n",
      "bert.encoder.layer.8.attention.self.query.bias cuda:0\n",
      "bert.encoder.layer.8.attention.self.key.weight cuda:0\n",
      "bert.encoder.layer.8.attention.self.key.bias cuda:0\n",
      "bert.encoder.layer.8.attention.self.value.weight cuda:0\n",
      "bert.encoder.layer.8.attention.self.value.bias cuda:0\n",
      "bert.encoder.layer.8.attention.output.dense.weight cuda:0\n",
      "bert.encoder.layer.8.attention.output.dense.bias cuda:0\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.8.intermediate.dense.weight cuda:0\n",
      "bert.encoder.layer.8.intermediate.dense.bias cuda:0\n",
      "bert.encoder.layer.8.output.dense.weight cuda:0\n",
      "bert.encoder.layer.8.output.dense.bias cuda:0\n",
      "bert.encoder.layer.8.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.8.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.9.attention.self.query.weight cuda:0\n",
      "bert.encoder.layer.9.attention.self.query.bias cuda:0\n",
      "bert.encoder.layer.9.attention.self.key.weight cuda:0\n",
      "bert.encoder.layer.9.attention.self.key.bias cuda:0\n",
      "bert.encoder.layer.9.attention.self.value.weight cuda:0\n",
      "bert.encoder.layer.9.attention.self.value.bias cuda:0\n",
      "bert.encoder.layer.9.attention.output.dense.weight cuda:0\n",
      "bert.encoder.layer.9.attention.output.dense.bias cuda:0\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.9.intermediate.dense.weight cuda:0\n",
      "bert.encoder.layer.9.intermediate.dense.bias cuda:0\n",
      "bert.encoder.layer.9.output.dense.weight cuda:0\n",
      "bert.encoder.layer.9.output.dense.bias cuda:0\n",
      "bert.encoder.layer.9.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.9.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.10.attention.self.query.weight cuda:0\n",
      "bert.encoder.layer.10.attention.self.query.bias cuda:0\n",
      "bert.encoder.layer.10.attention.self.key.weight cuda:0\n",
      "bert.encoder.layer.10.attention.self.key.bias cuda:0\n",
      "bert.encoder.layer.10.attention.self.value.weight cuda:0\n",
      "bert.encoder.layer.10.attention.self.value.bias cuda:0\n",
      "bert.encoder.layer.10.attention.output.dense.weight cuda:0\n",
      "bert.encoder.layer.10.attention.output.dense.bias cuda:0\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.10.intermediate.dense.weight cuda:0\n",
      "bert.encoder.layer.10.intermediate.dense.bias cuda:0\n",
      "bert.encoder.layer.10.output.dense.weight cuda:0\n",
      "bert.encoder.layer.10.output.dense.bias cuda:0\n",
      "bert.encoder.layer.10.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.10.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.11.attention.self.query.weight cuda:0\n",
      "bert.encoder.layer.11.attention.self.query.bias cuda:0\n",
      "bert.encoder.layer.11.attention.self.key.weight cuda:0\n",
      "bert.encoder.layer.11.attention.self.key.bias cuda:0\n",
      "bert.encoder.layer.11.attention.self.value.weight cuda:0\n",
      "bert.encoder.layer.11.attention.self.value.bias cuda:0\n",
      "bert.encoder.layer.11.attention.output.dense.weight cuda:0\n",
      "bert.encoder.layer.11.attention.output.dense.bias cuda:0\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.11.intermediate.dense.weight cuda:0\n",
      "bert.encoder.layer.11.intermediate.dense.bias cuda:0\n",
      "bert.encoder.layer.11.output.dense.weight cuda:0\n",
      "bert.encoder.layer.11.output.dense.bias cuda:0\n",
      "bert.encoder.layer.11.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.11.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.12.attention.self.query.weight cuda:0\n",
      "bert.encoder.layer.12.attention.self.query.bias cuda:0\n",
      "bert.encoder.layer.12.attention.self.key.weight cuda:0\n",
      "bert.encoder.layer.12.attention.self.key.bias cuda:0\n",
      "bert.encoder.layer.12.attention.self.value.weight cuda:0\n",
      "bert.encoder.layer.12.attention.self.value.bias cuda:0\n",
      "bert.encoder.layer.12.attention.output.dense.weight cuda:0\n",
      "bert.encoder.layer.12.attention.output.dense.bias cuda:0\n",
      "bert.encoder.layer.12.attention.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.12.attention.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.12.intermediate.dense.weight cuda:0\n",
      "bert.encoder.layer.12.intermediate.dense.bias cuda:0\n",
      "bert.encoder.layer.12.output.dense.weight cuda:0\n",
      "bert.encoder.layer.12.output.dense.bias cuda:0\n",
      "bert.encoder.layer.12.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.12.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.13.attention.self.query.weight cuda:0\n",
      "bert.encoder.layer.13.attention.self.query.bias cuda:0\n",
      "bert.encoder.layer.13.attention.self.key.weight cuda:0\n",
      "bert.encoder.layer.13.attention.self.key.bias cuda:0\n",
      "bert.encoder.layer.13.attention.self.value.weight cuda:0\n",
      "bert.encoder.layer.13.attention.self.value.bias cuda:0\n",
      "bert.encoder.layer.13.attention.output.dense.weight cuda:0\n",
      "bert.encoder.layer.13.attention.output.dense.bias cuda:0\n",
      "bert.encoder.layer.13.attention.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.13.attention.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.13.intermediate.dense.weight cuda:0\n",
      "bert.encoder.layer.13.intermediate.dense.bias cuda:0\n",
      "bert.encoder.layer.13.output.dense.weight cuda:0\n",
      "bert.encoder.layer.13.output.dense.bias cuda:0\n",
      "bert.encoder.layer.13.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.13.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.14.attention.self.query.weight cuda:0\n",
      "bert.encoder.layer.14.attention.self.query.bias cuda:0\n",
      "bert.encoder.layer.14.attention.self.key.weight cuda:0\n",
      "bert.encoder.layer.14.attention.self.key.bias cuda:0\n",
      "bert.encoder.layer.14.attention.self.value.weight cuda:0\n",
      "bert.encoder.layer.14.attention.self.value.bias cuda:0\n",
      "bert.encoder.layer.14.attention.output.dense.weight cuda:0\n",
      "bert.encoder.layer.14.attention.output.dense.bias cuda:0\n",
      "bert.encoder.layer.14.attention.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.14.attention.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.14.intermediate.dense.weight cuda:0\n",
      "bert.encoder.layer.14.intermediate.dense.bias cuda:0\n",
      "bert.encoder.layer.14.output.dense.weight cuda:0\n",
      "bert.encoder.layer.14.output.dense.bias cuda:0\n",
      "bert.encoder.layer.14.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.14.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.15.attention.self.query.weight cuda:0\n",
      "bert.encoder.layer.15.attention.self.query.bias cuda:0\n",
      "bert.encoder.layer.15.attention.self.key.weight cuda:0\n",
      "bert.encoder.layer.15.attention.self.key.bias cuda:0\n",
      "bert.encoder.layer.15.attention.self.value.weight cuda:0\n",
      "bert.encoder.layer.15.attention.self.value.bias cuda:0\n",
      "bert.encoder.layer.15.attention.output.dense.weight cuda:0\n",
      "bert.encoder.layer.15.attention.output.dense.bias cuda:0\n",
      "bert.encoder.layer.15.attention.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.15.attention.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.15.intermediate.dense.weight cuda:0\n",
      "bert.encoder.layer.15.intermediate.dense.bias cuda:0\n",
      "bert.encoder.layer.15.output.dense.weight cuda:0\n",
      "bert.encoder.layer.15.output.dense.bias cuda:0\n",
      "bert.encoder.layer.15.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.15.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.16.attention.self.query.weight cuda:0\n",
      "bert.encoder.layer.16.attention.self.query.bias cuda:0\n",
      "bert.encoder.layer.16.attention.self.key.weight cuda:0\n",
      "bert.encoder.layer.16.attention.self.key.bias cuda:0\n",
      "bert.encoder.layer.16.attention.self.value.weight cuda:0\n",
      "bert.encoder.layer.16.attention.self.value.bias cuda:0\n",
      "bert.encoder.layer.16.attention.output.dense.weight cuda:0\n",
      "bert.encoder.layer.16.attention.output.dense.bias cuda:0\n",
      "bert.encoder.layer.16.attention.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.16.attention.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.16.intermediate.dense.weight cuda:0\n",
      "bert.encoder.layer.16.intermediate.dense.bias cuda:0\n",
      "bert.encoder.layer.16.output.dense.weight cuda:0\n",
      "bert.encoder.layer.16.output.dense.bias cuda:0\n",
      "bert.encoder.layer.16.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.16.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.17.attention.self.query.weight cuda:0\n",
      "bert.encoder.layer.17.attention.self.query.bias cuda:0\n",
      "bert.encoder.layer.17.attention.self.key.weight cuda:0\n",
      "bert.encoder.layer.17.attention.self.key.bias cuda:0\n",
      "bert.encoder.layer.17.attention.self.value.weight cuda:0\n",
      "bert.encoder.layer.17.attention.self.value.bias cuda:0\n",
      "bert.encoder.layer.17.attention.output.dense.weight cuda:0\n",
      "bert.encoder.layer.17.attention.output.dense.bias cuda:0\n",
      "bert.encoder.layer.17.attention.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.17.attention.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.17.intermediate.dense.weight cuda:0\n",
      "bert.encoder.layer.17.intermediate.dense.bias cuda:0\n",
      "bert.encoder.layer.17.output.dense.weight cuda:0\n",
      "bert.encoder.layer.17.output.dense.bias cuda:0\n",
      "bert.encoder.layer.17.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.17.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.18.attention.self.query.weight cuda:0\n",
      "bert.encoder.layer.18.attention.self.query.bias cuda:0\n",
      "bert.encoder.layer.18.attention.self.key.weight cuda:0\n",
      "bert.encoder.layer.18.attention.self.key.bias cuda:0\n",
      "bert.encoder.layer.18.attention.self.value.weight cuda:0\n",
      "bert.encoder.layer.18.attention.self.value.bias cuda:0\n",
      "bert.encoder.layer.18.attention.output.dense.weight cuda:0\n",
      "bert.encoder.layer.18.attention.output.dense.bias cuda:0\n",
      "bert.encoder.layer.18.attention.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.18.attention.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.18.intermediate.dense.weight cuda:0\n",
      "bert.encoder.layer.18.intermediate.dense.bias cuda:0\n",
      "bert.encoder.layer.18.output.dense.weight cuda:0\n",
      "bert.encoder.layer.18.output.dense.bias cuda:0\n",
      "bert.encoder.layer.18.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.18.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.19.attention.self.query.weight cuda:0\n",
      "bert.encoder.layer.19.attention.self.query.bias cuda:0\n",
      "bert.encoder.layer.19.attention.self.key.weight cuda:0\n",
      "bert.encoder.layer.19.attention.self.key.bias cuda:0\n",
      "bert.encoder.layer.19.attention.self.value.weight cuda:0\n",
      "bert.encoder.layer.19.attention.self.value.bias cuda:0\n",
      "bert.encoder.layer.19.attention.output.dense.weight cuda:0\n",
      "bert.encoder.layer.19.attention.output.dense.bias cuda:0\n",
      "bert.encoder.layer.19.attention.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.19.attention.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.19.intermediate.dense.weight cuda:0\n",
      "bert.encoder.layer.19.intermediate.dense.bias cuda:0\n",
      "bert.encoder.layer.19.output.dense.weight cuda:0\n",
      "bert.encoder.layer.19.output.dense.bias cuda:0\n",
      "bert.encoder.layer.19.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.19.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.20.attention.self.query.weight cuda:0\n",
      "bert.encoder.layer.20.attention.self.query.bias cuda:0\n",
      "bert.encoder.layer.20.attention.self.key.weight cuda:0\n",
      "bert.encoder.layer.20.attention.self.key.bias cuda:0\n",
      "bert.encoder.layer.20.attention.self.value.weight cuda:0\n",
      "bert.encoder.layer.20.attention.self.value.bias cuda:0\n",
      "bert.encoder.layer.20.attention.output.dense.weight cuda:0\n",
      "bert.encoder.layer.20.attention.output.dense.bias cuda:0\n",
      "bert.encoder.layer.20.attention.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.20.attention.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.20.intermediate.dense.weight cuda:0\n",
      "bert.encoder.layer.20.intermediate.dense.bias cuda:0\n",
      "bert.encoder.layer.20.output.dense.weight cuda:0\n",
      "bert.encoder.layer.20.output.dense.bias cuda:0\n",
      "bert.encoder.layer.20.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.20.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.21.attention.self.query.weight cuda:0\n",
      "bert.encoder.layer.21.attention.self.query.bias cuda:0\n",
      "bert.encoder.layer.21.attention.self.key.weight cuda:0\n",
      "bert.encoder.layer.21.attention.self.key.bias cuda:0\n",
      "bert.encoder.layer.21.attention.self.value.weight cuda:0\n",
      "bert.encoder.layer.21.attention.self.value.bias cuda:0\n",
      "bert.encoder.layer.21.attention.output.dense.weight cuda:0\n",
      "bert.encoder.layer.21.attention.output.dense.bias cuda:0\n",
      "bert.encoder.layer.21.attention.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.21.attention.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.21.intermediate.dense.weight cuda:0\n",
      "bert.encoder.layer.21.intermediate.dense.bias cuda:0\n",
      "bert.encoder.layer.21.output.dense.weight cuda:0\n",
      "bert.encoder.layer.21.output.dense.bias cuda:0\n",
      "bert.encoder.layer.21.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.21.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.22.attention.self.query.weight cuda:0\n",
      "bert.encoder.layer.22.attention.self.query.bias cuda:0\n",
      "bert.encoder.layer.22.attention.self.key.weight cuda:0\n",
      "bert.encoder.layer.22.attention.self.key.bias cuda:0\n",
      "bert.encoder.layer.22.attention.self.value.weight cuda:0\n",
      "bert.encoder.layer.22.attention.self.value.bias cuda:0\n",
      "bert.encoder.layer.22.attention.output.dense.weight cuda:0\n",
      "bert.encoder.layer.22.attention.output.dense.bias cuda:0\n",
      "bert.encoder.layer.22.attention.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.22.attention.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.22.intermediate.dense.weight cuda:0\n",
      "bert.encoder.layer.22.intermediate.dense.bias cuda:0\n",
      "bert.encoder.layer.22.output.dense.weight cuda:0\n",
      "bert.encoder.layer.22.output.dense.bias cuda:0\n",
      "bert.encoder.layer.22.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.22.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.23.attention.self.query.weight cuda:0\n",
      "bert.encoder.layer.23.attention.self.query.bias cuda:0\n",
      "bert.encoder.layer.23.attention.self.key.weight cuda:0\n",
      "bert.encoder.layer.23.attention.self.key.bias cuda:0\n",
      "bert.encoder.layer.23.attention.self.value.weight cuda:0\n",
      "bert.encoder.layer.23.attention.self.value.bias cuda:0\n",
      "bert.encoder.layer.23.attention.output.dense.weight cuda:0\n",
      "bert.encoder.layer.23.attention.output.dense.bias cuda:0\n",
      "bert.encoder.layer.23.attention.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.23.attention.output.LayerNorm.bias cuda:0\n",
      "bert.encoder.layer.23.intermediate.dense.weight cuda:0\n",
      "bert.encoder.layer.23.intermediate.dense.bias cuda:0\n",
      "bert.encoder.layer.23.output.dense.weight cuda:0\n",
      "bert.encoder.layer.23.output.dense.bias cuda:0\n",
      "bert.encoder.layer.23.output.LayerNorm.weight cuda:0\n",
      "bert.encoder.layer.23.output.LayerNorm.bias cuda:0\n",
      "bert.pooler.dense.weight cuda:0\n",
      "bert.pooler.dense.bias cuda:0\n",
      "classifier.weight cuda:0\n",
      "classifier.bias cuda:0\n"
     ]
    }
   ],
   "source": [
    "for para in model.named_parameters():\n",
    "    print(para[0], para[1].device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7603f98",
   "metadata": {},
   "source": [
    "## pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14a08296",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T14:58:04.963526Z",
     "start_time": "2023-07-17T14:58:03.478944Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.checkpoint import checkpoint_sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf5c80e",
   "metadata": {},
   "source": [
    "## huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5671adb1",
   "metadata": {},
   "source": [
    "- checkpoint default module\n",
    "\n",
    "```\n",
    "class BertPreTrainedModel(PreTrainedModel):\n",
    "    def _set_gradient_checkpointing(self, module, value=False):\n",
    "        # 只对 BertEncoder 进行 checkpoint\n",
    "        if isinstance(module, BertEncoder):\n",
    "            module.gradient_checkpointing = value\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff62430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
