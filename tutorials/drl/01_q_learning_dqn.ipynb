{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9f05039",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T14:00:15.960018Z",
     "start_time": "2023-11-02T14:00:15.951709Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70929104",
   "metadata": {},
   "source": [
    "## basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4c3eb3",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "U_t&=R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2} + \\cdots + \\gamma^{n-t} R_n\\\\\n",
    "&=\\sum_{k=t}^n\\gamma^{k-t}R_k{}\\\\\n",
    "Q_\\pi(s_t,a_t)&=\\mathbb E\\left[U_t|S_t=s_t,A_t=a_t\\right]\\\\\n",
    "&=\\mathbb E_{S_{t+1},A_{t+1},\\cdots, S_n,A_n}\\left[U_t|S_t=s_t,A_t=a_t\\right]\\\\\n",
    "Q_\\star(s_t,a_t)&=\\max_\\pi Q_\\pi(s_t,a_t)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- $U_t$: discounted return,\n",
    "    - return: cumulative future reward\n",
    "- $Q_\\pi(s_t,a_t)$：action value function，动作价值函数\n",
    "    - 其计算式中的期望消除了 $t$ 时刻之后的所有状态 $S_{t+1},\\cdots, S_n$ 与所有动作 $A_{t+1}, \\cdots, A_n$ 的随机性（也是 $U_t$ 随机性的来源）\n",
    "        - 未来时刻的 $S_{t+1}, A_{t+1}$ 会带来 $R_{t+1}$\n",
    "    - 对谁求期望就是消除对谁的随机性；\n",
    "- $Q_\\star(s_t,a_t)$：最优动作价值函数；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f2b7aa",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494b8d76",
   "metadata": {},
   "source": [
    "$$\n",
    "Q(s,a;w)\n",
    "$$\n",
    "\n",
    "- $w$ 表示 NN 的模型参数；\n",
    "- DQN 的输入是 $s$，输出是离散动作空间 $\\mathcal A$ 中的每个动作的 Q 值，是一个 scalar value；\n",
    "- train DQN 的目标即是，对所有的 $s,a$ pair， DQN 对 $Q(s,a;w)$ 的预测尽量接近 $Q_\\star(s,a)$\n",
    "- 训练 DQN 最常用的算法是 TD（temporal difference，时间差分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22d5fb1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T15:11:50.087607Z",
     "start_time": "2023-11-02T15:11:50.077896Z"
    }
   },
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self, dim_state, num_action):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim_state, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_action)\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579ae38a",
   "metadata": {},
   "source": [
    "## TD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a001b591",
   "metadata": {},
   "source": [
    "- TD target 与 TD error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55e65d9",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "U_t&=R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2} + \\cdots + \\gamma^{n-t} R_n\\\\\n",
    "&=\\sum_{k=t}^n\\gamma^{k-t}R_k\\\\\n",
    "&=R_{t}+\\sum_{k=t+1}^n\\gamma^{k-t}R_{k}\\\\\n",
    "&=R_{t}+\\gamma\\sum_{k=t+1}^n\\gamma^{k-(t+1)}R_{k}\\\\\n",
    "&=R_{t}+\\gamma U_{t+1}\\\\\n",
    "Q_\\star(s_t,a_t)&=\\max_\\pi\\mathbb E\\left[U_t|S_t=s_t,A_t=a_t\\right]\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- $U_t$ 的递归定义与计算\n",
    "- 基于上述的两个等式可以推导出著名的最优贝尔曼方程（optimal Bellman equation）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180802c3",
   "metadata": {},
   "source": [
    "### optimal Bellman equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a9ac6",
   "metadata": {},
   "source": [
    "$$\n",
    "Q_\\star(s_t,a_t)=\\mathbb E_{S_{t+1}\\sim p(\\cdot|s_t,a_t)}\\left[R_t+\\gamma\\cdot\\max_{\\pi}Q_\\star(S_{t+1}, A)\\bigg|S_t=s_t,A_t=a_t\\right]\n",
    "$$\n",
    "\n",
    "- 也是一种 recursive 的计算方式\n",
    "- 右侧是一个期望，期望可以通过蒙特卡洛方法近似，当 Agent 在状态 $s_t$ 执行动作 $a_t$ 之后，环境通过状态转移函数 $p(s_{t+1}|s_t,a_t)$（mdp）计算出下一时刻的状态 $s_{t+1}$，因此当我们观测到 $s_t,a_t,s_{t+1}$ 时，奖励 $R_t$ 也会被观测到，记作 $r_t$，于是有了如下的四元组\n",
    "\n",
    "$$\n",
    "(s_t,a_t,r_t,s_{t+1})\n",
    "$$\n",
    "\n",
    "- 基于蒙特卡洛估计，可以算出等式右边期望的近似\n",
    "\n",
    "$$\n",
    "Q_\\star(s_t,a_t) \\approx r_t+\\gamma\\cdot \\max_{a\\in\\mathcal A}Q_\\star(s_{t+1},a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f3b1b8",
   "metadata": {},
   "source": [
    "### 基于 TD train DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5268db55",
   "metadata": {},
   "source": [
    "- 将最优动作价值函数 $Q_\\star(s_t,a_t)$ 替换为神经网络 $Q(s,a;w)$\n",
    "\n",
    "$$\n",
    "\\underbrace{Q(s_t,a_t;w)}_{预测 \\hat{q}_t}\\approx \\underbrace{r_t + \\gamma\\cdot \\max_{a\\in\\mathcal A}Q(s_{t+1},a;w)}_{\\text{TD target}, \\hat y_t}\n",
    "$$\n",
    "\n",
    "- 左边的 $Q(s_t,a_t;w)$ 是神经网络在 $t$ 时刻做出的预测：$\\hat q_t$\n",
    "- 右边的 TD target 则是神经网络在 $t+1$ 时刻做出的预测：$\\hat y_t$，且基于了真实观测到的奖励 $r_t$（多了一部分事实）\n",
    "- 对于右式 $\\hat y_t$ 比着左式 $\\hat q_t$ 多了部分事实，因此更可信，**在监督学习的范式下**，可以作为训练的 groud truth（将其视为常数），定义如下的损失\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "L(w)&=\\frac12\\left[Q(s_t,a_t;w)-\\hat y_t\\right]^2=\\frac12\\left[\\hat q_t-\\hat y_t\\right]^2\\\\\n",
    "\\nabla_wL(w)&=\\underbrace{(\\hat q_t-\\hat y_t)}_{\\text{TD error}}\\cdot \\nabla_wQ(s_t,a_t;w)=\\delta_t\\cdot \\nabla_wQ(s_t,a_t;w)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- 梯度下降优化让 $\\hat q_t$ 更接近 $\\hat y_t$\n",
    "\n",
    "$$\n",
    "w\\leftarrow w-\\alpha\\cdot\\delta_t\\nabla_wQ(s_t,a_t;w)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ceeab",
   "metadata": {},
   "source": [
    "### training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0c72da",
   "metadata": {},
   "source": [
    "- 给定一个四元组 $(s_t,a_t,r_t,s_{t+1})$ 一个 DQN 网络，我们可以得到\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\hat q_t&=Q(s_t,a_t;w)\\\\\n",
    "\\hat y_t&=r_t+\\max_{a\\in\\mathcal A}Q(s_{t+1},a;w)\\\\\n",
    "\\delta_t&=\\hat q_t-\\hat y_t\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- 梯度下降优化让 $\\hat q_t$ 更接近 $\\hat y_t$\n",
    "\n",
    "$$\n",
    "w\\leftarrow w-\\alpha\\cdot\\delta_t\\nabla_wQ(s_t,a_t;w)\n",
    "$$\n",
    "\n",
    "\n",
    "- 训练 DQN 所需的四元组数据 $(s_t,a_t,r_t,s_{t+1})$ 与控制 agent 运动的策略无关，意味着可以用任何策略控制智能体与环境交互，同时记录下算法运动轨迹（trajectory），作为 DQN 的训练数据；\n",
    "- 因此 DQN 的训练可以分为两个独立的部分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f9b8f4",
   "metadata": {},
   "source": [
    "#### 收集训练数据    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e6a795",
   "metadata": {},
   "source": [
    "- 可以用任意策略 $\\pi$ 来控制智能体与环境交互，此时的 $\\pi$ 可以称为行为策略，一种经典的行为策略是 $\\epsilon$-greedy 策略\n",
    "        \n",
    "$$\n",
    "a_t=\\begin{cases}\n",
    "\\arg\\max_a Q(s_t,a;w), & r < 1-\\epsilon (以概率 (1-\\epsilon), )\\\\\n",
    "均匀采样 a\\in \\mathcal A, & r < \\epsilon  (以概率 \\epsilon)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- 将 Agent 在一个 episode （回合） 中的轨迹记作\n",
    "\n",
    "$$\n",
    "(s_1,a_1,r_1), (s_2,a_2,r_2), (s_3,a_3,r_3), \\cdots, (s_n,a_n,r_n)\n",
    "$$\n",
    "\n",
    "- 进一步将其划分为 $(s_t,a_t,r_t,s_{t+1})$ 这样的四元组，存入缓存，这个缓存就叫经验回放缓存（Experience Replay Buffer）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6b0dd86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T14:56:14.170445Z",
     "start_time": "2023-11-02T14:56:14.148938Z"
    }
   },
   "outputs": [],
   "source": [
    "# 循环队列\n",
    "# 两个核心函数：\n",
    "# 1. push 队列中循环添加\n",
    "# 2. sample：队列中采样\n",
    "\n",
    "@dataclass\n",
    "class ReplayBuffer:\n",
    "    maxszie: int\n",
    "    size: int = 0\n",
    "        \n",
    "    # s_t\n",
    "    states: list = field(default_factory=list)\n",
    "    # a_t\n",
    "    actions: list = field(default_factory=list)\n",
    "    # r_t\n",
    "    rewards: list = field(default_factory=list)\n",
    "    # s_{t+1}\n",
    "    next_states: list = field(default_factory=list)\n",
    "    \n",
    "    dones: list = field(default_factory=list)\n",
    "    \n",
    "    def push(self, state, action, reward, done, next_state):\n",
    "        if self.size < self.maxsize:\n",
    "            self.states.append(state)\n",
    "            self.actions.append(action)\n",
    "            self.rewards.append(reward)\n",
    "            self.dones.append(done)\n",
    "            self.next_states.append(next_state)\n",
    "        else:\n",
    "            # overlap\n",
    "            index = self.size % self.maxsize\n",
    "            self.states[index] = state\n",
    "            self.actions[index] = action\n",
    "            self.rewards[index] = reward\n",
    "            self.dones[index] = done\n",
    "            self.next_states[index] = next_state\n",
    "        self.size += 1\n",
    "    \n",
    "    def sample(self, n):\n",
    "        total_number = min(self.size, self.maxsize)\n",
    "        indices = np.random.randint(total_number, size=n)\n",
    "        sample_states = [self.states[ind] for ind in indices]\n",
    "        sample_actions = [self.actions[ind] for ind in indices]\n",
    "        sample_rewards = [self.rewards[ind] for ind in indices]\n",
    "        sample_dones = [self.dones[ind] for ind in indices]\n",
    "        sample_next_states = [self.next_states[ind] for ind in indices]\n",
    "        return sample_states, sample_actions, sample_rewards, sample_dones, sample_next_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab95fd1e",
   "metadata": {},
   "source": [
    "####  `forward/backward` 监督训练过程\n",
    "\n",
    "> 从 Experience Replay Buffer 中随机选出一个四元组，记作 $(s_j,a_j,r_j,s_{j+1})$，假定 DQN 当前的参数为 $w_{now}$，经过如下的 forward/backward 步骤对参数更新，得到 $w_{new}$\n",
    "\n",
    "\n",
    "1. forward：分别输入 $s_j$ 和 $s_{j+1}$（两个都是标量值）\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\hat q_j&=Q(s_j,a_j;w_{now})\\\\\n",
    "\\hat q_{j+1}&=\\max_{a\\in\\mathcal A}Q(s_{j+1},a;w_{now})\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "2. 计算 TD target 与 TD error，以及定义 loss\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\hat y_j&=r_j+\\gamma \\hat q_{j+1}, \\\\\n",
    "\\delta_j&=\\hat q_j-\\hat y_j\\\\\n",
    "L(w)&=\\frac12 \\delta_j^2=\\frac12\\left(\\hat q_j-\\hat y_j\\right)^2\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "3. loss 反向传播，计算梯度（`loss.backward()` 自动计算）\n",
    "\n",
    "$$\n",
    "g_j=\\nabla_wQ(s_j,a_j;w_{now})\n",
    "$$\n",
    "\n",
    "4. `optimizer.step()` 基于梯度下降更新参数\n",
    "\n",
    "$$\n",
    "w_{new}\\Leftarrow w_{old}-\\alpha\\cdot\\delta_j\\cdot g_j\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc7a1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, dim_state, num_action, gamma=0.9):\n",
    "        self.gamma = gamma\n",
    "        self.Q = QNet(dim_state, num_action)\n",
    "        self.target_Q = QNet(dim_state, num_action)\n",
    "        self.target_Q.load_state_dict(self.Q.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
