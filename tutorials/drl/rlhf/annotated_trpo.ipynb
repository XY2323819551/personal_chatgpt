{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8485461d-4a06-449f-9362-64b162752a5e",
   "metadata": {},
   "source": [
    "- references\n",
    "    - https://github.com/DeepRLChinese/DeepRL-Chinese/blob/master/09_trpo.py\n",
    "- 对于 DRL 而言\n",
    "    - 神经网络反而是简单的，就是一个超强的 function approximator；训练一个 deep neural network，就是学习一个函数近似器\n",
    "        - $\\pi_\\theta(\\cdot|s)=\\pi_\\theta(a|s)$\n",
    "        - $V(s)$\n",
    "    - 且在 DRL 的问题及应用里，我们需要更灵活多样地组织 learning/training 的 pipeline；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b6cc61-c879-4362-b8fe-5d25fbc372b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from collections import Counter\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c83a9535-4eb6-459e-a2c7-b080eaaa19f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 147, 0: 121, 1: 119, 2: 113})\n",
      "Counter({0: 137, 2: 130, 3: 117, 1: 116})\n"
     ]
    }
   ],
   "source": [
    "# 类别型概率分布的(依概率)采样\n",
    "c = Counter()\n",
    "m = Categorical(probs=torch.tensor([0.25, 0.25, 0.25, 0.25]))\n",
    "for _ in range(500):\n",
    "    c.update([m.sample().numpy().tolist()])\n",
    "print(c)\n",
    "print(Counter(m.sample((500, )).numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32f284ce-21c7-47d6-8826-ed02d088933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a layer that outputs a multinomial distribution\n",
    "    Methods\n",
    "    ------\n",
    "    __call__(log_action_probs)\n",
    "        Takes as input log probabilities and outputs a pytorch multinomail\n",
    "        distribution\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, log_action_probs):\n",
    "        return Categorical(logits=log_action_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "994433f1-8d3c-413b-b163-4baa1b6b741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, dim_obs, num_act):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim_obs, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_act)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "        self.categorical = CategoricalLayer()\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = F.relu(self.fc1(obs))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # logits\n",
    "        x = self.log_softmax(x)\n",
    "        x = self.categorical(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9428c747-b26a-4e27-b6f3-ede0ee0424c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, dim_obs):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim_obs, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = F.relu(self.fc1(obs))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35936cc1-d2a1-43dc-96bc-7b29e1387eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRPO 实例，不再是一个纯 nn 的 model\n",
    "# 而是一个 agent\n",
    "# pi: s -> a\n",
    "class TRPO:\n",
    "    def __init__(self, args):\n",
    "        self.discount = args.discount\n",
    "        \n",
    "        self.policy_net = PolicyNet(args.dim_obs, args.num_act)\n",
    "        self.value_net = ValueNet(args.dim_obs)\n",
    "\n",
    "        # 这里我们就可以看出 policy net 与 value net 训练的一个差异\n",
    "        self.value_optimizer = torch.optim.AdamW(self.value_net.parameters(), lr=args.lr_value_net)\n",
    "\n",
    "        # 最大化过程依赖的一些超参数\n",
    "        self.max_kl_div = 0.01\n",
    "        self.cg_max_iters = 10\n",
    "        self.line_search_accept_ratio = 0.1\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        # pi(a|s)：这是一个概率模型，\n",
    "        action_dist = self.policy_net(obs)\n",
    "        # 依概率采样\n",
    "        act = action_dist.sample()\n",
    "        return act"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
