{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d977214f-9889-4c4a-88d3-e3a0bdac3efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbfaa44-0cdc-40b4-9d07-a571c30f3b84",
   "metadata": {},
   "source": [
    "## trpo vs. policy gradient "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e97bd97-117d-421b-b381-166ef2db8e4b",
   "metadata": {},
   "source": [
    "- cons:\n",
    "    - 计算量更大；\n",
    "- pros:\n",
    "    - 表现更稳定，收敛更快\n",
    "- trust region：\n",
    "    - 数值优化领域很经典的算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc8b852-4625-4218-ada2-cdd6c96d397a",
   "metadata": {},
   "source": [
    "### Gradient Ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42469ebc-6134-4c27-9b24-a31fdbace529",
   "metadata": {},
   "source": [
    "- gradient ascent（最大化问题梯度上升，如果是最小化问题，则用梯度下降）\n",
    "\n",
    "    $$\n",
    "    \\theta^\\star=\\arg\\max_\\theta J(\\theta)\n",
    "    $$\n",
    "\n",
    "    - $g=\\frac{\\partial J(\\theta)}{\\partial \\theta}\\big|_{\\theta=\\theta_{old}}$，在 $\\theta_{old}$ 时，计算梯度；\n",
    "    - $\\theta_{new}=\\theta_{old}+\\alpha\\cdot g$，基于梯度上升，更新参数；\n",
    "    - 直到梯度 $g$ 的二范数接近于0；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b00abe-9a43-44c2-a451-d492669dd799",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Ascent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471b3ef9-3877-47f8-996f-112e20c4ae20",
   "metadata": {},
   "source": [
    "- stochastic gradient ascent\n",
    "    - $J(\\theta)=\\mathbb E_S[V(S;\\theta)]$ （$S$ 是随机变量）\n",
    "        - 期望需要做定积分，但定积分可能没有解析解，也就是求不出期望，也就不可能算出梯度；\n",
    "        - 求不出梯度，但可以求出随机梯度（stochastic gradient），随机梯度是对真实梯度的蒙特卡洛近似，\n",
    "        - 用随机梯度代替梯度得到的算法就是随机梯度上升（stochastic gradient ascent）；\n",
    "    - SGA\n",
    "        - $s \\Leftarrow$  random sampling\n",
    "        - $g=\\frac{\\partial V(s;\\theta)}{\\partial \\theta}\\big|_{\\theta=\\theta_{old}}$（随机梯度）\n",
    "        - $\\theta_{new}=\\theta_{old}+\\alpha\\cdot g$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05916b89-5fd8-4c8c-bb8f-756396a208a7",
   "metadata": {},
   "source": [
    "### Trust Region （置信域）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d502e0d-c146-4259-8b34-11f7423a63bf",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta^\\star=\\arg\\max_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "- 定义 $\\mathcal N(\\theta_{old})$ 是 $\\theta_{old}$ 的邻域；\n",
    "    - 半径为 $\\delta$ 的圆；\n",
    "  $$\n",
    "  \\mathcal N(\\theta_{old})=\\left\\{\\theta\\big| \\|\\theta-\\theta_{old}\\|\\leq \\delta\\right\\}\n",
    "  $$\n",
    "\n",
    "- 如果相比较复杂的 $J(\\theta)$, $L(\\theta|\\theta_{old})$ 是一个更为简单的函数，且 $L(\\theta|\\theta_{old})$ 能更好地逼近 $J(\\theta)$ 在 $\\mathcal N(\\theta_{old})$，则 $\\mathcal N(\\theta_{old})$ 就可以成为置信域；\n",
    "    - 用 $L(\\theta|\\theta_{old})$ 代替 $J(\\theta)$ 可以让优化变得更容易；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3af79c-d628-425b-a540-4b2d59d53316",
   "metadata": {},
   "source": [
    "- 置信域算法（Trust Regions）重复如下的两步\n",
    "    - Approximation（做近似）：给定 $\\theta_{old}$ 构造 $L(\\theta|\\theta_{old})$ 是在 $\\mathcal N(\\theta_{old})$ 内对 $J(\\theta)$ 的近似\n",
    "        - 比如用 $J(\\theta)$ 的二阶泰勒展开\n",
    "        - 比如用期望的蒙特卡洛近似\n",
    "    - Maximization（最大化）：$\\theta_{new}\\leftarrow \\arg\\max_{\\theta\\in\\mathcal N(\\theta_{old})} L(\\theta|\\theta_{old})$\n",
    "        - 带约束的最大化问题\n",
    "            - 置信域的半径可以发生，我们可以让半径逐渐变小；\n",
    "        - 每一轮都要求解这样一个最大化问题，因此置信域的算法的求解速度不快\n",
    "        - 运算量会比 gd/sgd 要大，其好处在于比梯度算法更稳定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddcc1e08-08a7-448e-a4d1-c333292c4cc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/cc/Mmalgorithm.jpg\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://upload.wikimedia.org/wikipedia/commons/c/cc/Mmalgorithm.jpg', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a55cc58-458d-4581-8900-1691252731cf",
   "metadata": {},
   "source": [
    "- $\\theta_m$ 的邻域就是 trust region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727e7117-b233-44a6-a4bd-ce84d4c91ba8",
   "metadata": {},
   "source": [
    "## policy-based reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1aaee8-c684-42d6-9fd6-78b67cc10b3a",
   "metadata": {},
   "source": [
    "- $\\pi_\\theta(a|s)$ policy network, controlling the agent\n",
    "- $V_\\pi(s)=\\mathbb E_{A\\in \\pi}[Q_\\pi(s,A)]$，状态价值函数（state-value function）\n",
    "    - 对谁求积分，就是消掉谁的过程，比如下面的 $A$\n",
    "$$\n",
    "\\begin{split}\n",
    "V_\\pi(s)&=\\mathbb E_{A\\in\\pi}[Q_\\pi(s,A)]\\\\\n",
    "&=\\sum_a \\pi_\\theta(a|s) Q_\\pi(s,a)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- objective function\n",
    "  - 对谁求积分，就是消掉谁的过程，比如下面的 $S$\n",
    "\n",
    "$$\n",
    "J(\\theta)=\\mathbb E_S[V_\\pi(S)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9139f7e5-3dba-453b-a83b-74b5d83a1c58",
   "metadata": {},
   "source": [
    "- objective function 的推导\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "V_\\pi(S)&=\\sum_a\\pi_\\theta(a|s)Q_\\pi(s,a)\\\\\n",
    "&=\\sum_a\\pi_{\\theta_{old}}(a|s)\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{old}}(a|s)}Q_\\pi(s,a)\\\\\n",
    "&=\\mathbb E_{A\\sim\\pi_{\\theta_{old}}(\\cdot|s)}\\left[\\frac{\\pi_\\theta(A|s)}{\\pi_{\\theta_{old}}(A|s)}Q_\\pi(s,A)\\right]\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- trpo 中最重要的公式：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "J(\\theta)&=\\mathbb E_S[V_\\pi(S)]\\\\\n",
    "&=\\mathbb E_S\\left[\\mathbb E_{A\\sim\\pi_{\\theta_{old}}(\\cdot|s)}\\left[\\frac{\\pi_\\theta(A|s)}{\\pi_{\\theta_{old}}(A|s)}Q_\\pi(s,A)\\right]\\right]\\\\\n",
    "&=\\mathbb E_S\\left[\\mathbb E_{A}\\left[\\frac{\\pi_\\theta(A|s)}{\\pi_{\\theta_{old}}(A|s)}Q_\\pi(s,A)\\right]\\right]\\\\\n",
    "&=\\mathbb E_{S,A}\\left[\\frac{\\pi_\\theta(A|s)}{\\pi_{\\theta_{old}}(A|s)}Q_\\pi(s,A)\\right]\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- $S$ 的随机性来自于状态转移（$S$ is sampled from the **state transition** of the env)\n",
    "- $A$ 的随机性来自于 $\\pi_{\\theta_{old}}(A|S)$（$A$ is sampled from $\\pi_{\\theta_{old}}(A|S)$）\n",
    "- 对期望 $\\mathbb E_{S,A}$ 做蒙特卡洛近似；\n",
    "    - trajectory（基于 $\\pi_{\\theta_{old}}(\\cdot|s)$）：$s_1,a_1,r_1,...,s_n,a_n,r_n$\n",
    "    - 蒙特卡洛近似（Step 1，approximation）\n",
    " \n",
    "      $$\n",
    "      L(\\theta|\\theta_{old})=\\frac1n\\sum_{i=1}^n\\frac{\\pi_\\theta(a_i|s_i)}{\\pi_{\\theta_{old}}(a_i|s_i)}Q_\\pi(s_i,a_i)\n",
    "      $$\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa7ffdc-1b65-484b-ba72-cfb03af34e48",
   "metadata": {},
   "source": [
    "### TRPO 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127a11d0-2229-4b7f-b801-a113ca2e105a",
   "metadata": {},
   "source": [
    "- 巧妙地将数值优化中的 trust region 应用在策略网络 $\\pi_\\theta(a|s)$ 学习的优化中\n",
    "- policy gradient：\n",
    "    - 对参数敏感；\n",
    "    - 收敛曲线上下震荡；\n",
    "- more robust than policy gradient algorithms\n",
    "- more sample efficient than policy gradient algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92a98c4-5b40-4932-9505-761e110be41b",
   "metadata": {},
   "source": [
    "- Step 1，approximation\n",
    "\n",
    "  $$\n",
    "  L(\\theta|\\theta_{old})=\\frac1n\\sum_{i=1}^n\\frac{\\pi_\\theta(a_i|s_i)}{\\pi_{\\theta_{old}}(a_i|s_i)}Q_\\pi(s_i,a_i)\n",
    "  $$\n",
    "\n",
    "    - 进一步对 $Q_\\pi(s_i,a_i)$ 做蒙特卡洛近似\n",
    "        - 一次 episode 观测到的 rewards：$r_1,r_2,\\cdots,r_n$\n",
    "        - 计算 discounted return（$i$ 时刻起，所有未来时刻的奖励的加权和）\n",
    "\n",
    "          $$\n",
    "          u_i=r_i+\\gamma r_{i+1}+\\gamma^2r_{i+2}+\\cdots+\\gamma^{n-i}r_n=\\sum_{k=i}^n\\gamma^{k-i}r_k\n",
    "          $$\n",
    "        - $Q_\\pi(s_i,a_i)\\approx u_i$\n",
    "\n",
    "  $$\n",
    "  \\begin{split}\n",
    "  J(\\theta)&\\approx L(\\theta|\\theta_{old})=\\frac1n\\sum_{i=1}^n\\frac{\\pi_\\theta(a_i|s_i)}{\\pi_{\\theta_{old}}(a_i|s_i)}Q_\\pi(s_i,a_i)\\\\\n",
    "  &\\approx \\hat L(\\theta|\\theta_{old})=\\frac1n\\sum_{i=1}^n\\frac{\\pi_\\theta(a_i|s_i)}{\\pi_{\\theta_{old}}(a_i|s_i)}u_i\n",
    "  \\end{split}\n",
    "  $$\n",
    "\n",
    "- Step 2, maximization\n",
    "\n",
    "    $$\n",
    "    \\theta_{new}\\leftarrow \\arg\\max_\\theta \\hat L(\\theta|\\theta_{old}), \\quad s.t. \\theta\\in \\mathcal N(\\theta_{old})\n",
    "    $$\n",
    "\n",
    "    - 如何衡量 $\\theta$ 与 $\\theta_{old}$ 的距离\n",
    "        - $\\|\\theta-\\theta_{old}\\|\\leq \\Delta$\n",
    "        - $\\frac1n\\sum_{i=1}^nKL[\\pi_{\\theta_{old}}(\\cdot|s_i)\\|\\pi_{\\theta}(\\cdot|s_i)]\\leq \\Delta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8e82018-462e-4bed-8f28-e4dba74b1690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../imgs/trpo_summary.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='../imgs/trpo_summary.png', width=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
