{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff20d5f2",
   "metadata": {},
   "source": [
    "## basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019c297",
   "metadata": {},
   "source": [
    "- policy function: $\\pi(a|s)$ is a PDF\n",
    "    - $\\sum_{a}\\pi(a|s)=1$\n",
    "- Policy Network: $\\pi(a|s;\\mathbf \\theta)$\n",
    "    - $\\bf\\theta$ 表示神经网络的待训练参数\n",
    "    - $\\sum_{a}\\pi(a|s;\\theta)=1$（last layer of nn，称之为 logits，接一个 softmax 的变换）\n",
    "- 状态价值函数（state value function），对其做函数近似（function approximation）；\n",
    "\n",
    "    - **Discounted reward**\n",
    "\n",
    "        $$\n",
    "        U_t=R_t+\\gamma R_{t+1} + \\gamma^2R_{t+2} + \\gamma^3R_{t+3}+\\cdots\n",
    "        $$\n",
    "\n",
    "        - $U_t$ 的精确计算依赖 $A_t,A_{t+1},A_{t+2}, \\cdots, $ 和 $S_t,S_{t+1},S_{t+2},\\cdots$\n",
    "        - $t$ 时刻，未来的 reward 为还未观测到的随机变量，\n",
    "            - 每个奖励 $R_t$ 的随机性都来自于前一个时刻的动作 $A_t$ 和 $S_t$\n",
    "            - 动作的随机性来自于策略函数 $\\pi(a|\\cdot)$，状态的随机性来自于状态转移函数 $p(s_t|s_{t-1})$\n",
    "    - **action-value function**\n",
    "        $$\n",
    "        Q_\\pi(s_t,a_t)=\\mathbb E[U_t|S_t=s_t,A_t=a_t]\n",
    "        $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21f0a0a",
   "metadata": {},
   "source": [
    "### 策略学习的目标函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa59810a",
   "metadata": {},
   "source": [
    "- $S_t,A_t,S_{t+1},A_{t+1}, S_{t+2}, A_{t+2}$\n",
    "- $Q_\\pi(s_t,a_t)=\\mathbb E[U_t|S_t=s_t,A_t=a_t]$\n",
    "- $V_\\pi(s_t)=\\mathbb E_{A_t\\sim \\pi(\\cdot|s_t;\\theta)}Q_\\pi(s_t,A_t)$\n",
    "- 状态价值（state value）既依赖当前状态 $s_t$ 也依赖策略网络 $\\pi$ 的参数 $\\theta$\n",
    "    - 当前状态 $s_t$ 越好，$V_\\pi(s_t)$ 越大，回报 $U_t$ 的期望也就越大；\n",
    "    - 策略网络 $\\pi$ 越好，$V_\\pi(s_t)$ 也会越大；\n",
    "- 定义目标函数\n",
    "\n",
    "    $$\n",
    "    \\begin{split}\n",
    "    &J(\\theta)=\\mathbb E_S[V_\\pi(S)]\\\\\n",
    "    &\\max_{\\theta} J(\\theta)\n",
    "    \\end{split}\n",
    "    $$\n",
    "    \n",
    "    - 最大化目标，通过梯度上升来优化\n",
    "    \n",
    "    $$\n",
    "    \\theta\\leftarrow \\theta+\\eta \\frac{\\partial J(\\theta)}{\\partial \\theta}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b61a2a",
   "metadata": {},
   "source": [
    "### 策略梯度定理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027d32e9",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta}=\\mathbb E_S\\left[\\mathbb E_{A\\sim \\pi(\\cdot|S;\\theta)}\\left[\\frac{\\partial \\ln_\\pi(A|S;\\theta)}{\\partial \\theta}\\cdot Q_\\pi(S,A)\\right]\\right]\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
